{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "qa.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thingumajig/colab-experiments/blob/master/qa.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vC1s1yF99t5M",
        "colab_type": "text"
      },
      "source": [
        "# Init"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tgUEga5G9y_w",
        "colab_type": "text"
      },
      "source": [
        "## Install packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IiEQ3rLjNRzG",
        "colab_type": "code",
        "outputId": "b34a7a7c-1c53-49b9-8e57-5a0a9a0cf2ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 416
        }
      },
      "source": [
        "!pip install spacy\n",
        "\n",
        "!pip3 uninstall --quiet --yes tensorflow\n",
        "!pip3 install --quiet tensorflow-gpu==1.14.0\n",
        "!pip3 install --quiet tensorflow-hub\n",
        "!pip3 install --quiet sentencepiece==0.1.83\n",
        "!pip3 install --quiet tf-sentencepiece==0.1.83\n",
        "!pip3 install --quiet simpleneighbors\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.6/dist-packages (2.1.8)\n",
            "Requirement already satisfied: preshed<2.1.0,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.0.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.21.0)\n",
            "Requirement already satisfied: thinc<7.1.0,>=7.0.8 in /usr/local/lib/python3.6/dist-packages (from spacy) (7.0.8)\n",
            "Requirement already satisfied: srsly<1.1.0,>=0.0.6 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.1.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.17.3)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.2)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.3.0)\n",
            "Requirement already satisfied: blis<0.3.0,>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.2.4)\n",
            "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.9.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2019.9.11)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<7.1.0,>=7.0.8->spacy) (4.28.1)\n",
            "\u001b[K     |████████████████████████████████| 377.0MB 43kB/s \n",
            "\u001b[K     |████████████████████████████████| 3.2MB 28.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 491kB 51.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.0MB 6.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 2.7MB 6.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 645kB 8.8MB/s \n",
            "\u001b[?25h  Building wheel for annoy (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r5oCPLvd-AQZ",
        "colab_type": "text"
      },
      "source": [
        "## Initialize grammar parser"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4weL6rPINbY5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "# doc = nlp(\"With respect to all losses caused by the peril of Flood, the Company shall not be liable, in the aggregate for any one Policy year, for more than its proportionate share of US$25,000,000.\")\n",
        "# doc = nlp(\"The Program limit of liability is US$400,000,000.\")\n",
        "# print(doc)\n",
        "# for token in doc:\n",
        "#     print(\"{2}-{1}({3}-{6}, {0}-{5})\".format(token.text, token.tag_, token.dep_, token.head.text, token.head.tag_, token.i+1, token.head.i+1))\n",
        "# for np in doc.noun_chunks:\n",
        "#   print(np.text)\n",
        "\n",
        "\n",
        "from spacy.symbols import *\n",
        "\n",
        "np_labels = set([nsubj, nsubjpass, dobj, iobj, pobj]) # Probably others too\n",
        "np_labels_full = set([nsubj, nsubjpass, dobj, iobj, pobj, csubj, csubjpass, attr]) # Probably others too\n",
        "# print(dir(spacy.symbols))\n",
        "# subj - subject\n",
        "# nsubj - nominal subject\n",
        "# nsubjpass - passive nominal subject\n",
        "# csubj - clausal subject\n",
        "# csubjpass - passive clausal subject\n",
        "\n",
        "def iter_nps(doc):\n",
        "    for word in doc:\n",
        "        if word.dep in np_labels_full:\n",
        "            yield word\n",
        "\n",
        "def iter_nps_str(doc):\n",
        "  s = ''\n",
        "  for np in iter_nps(doc):\n",
        "    for t in np.subtree:\n",
        "      s += str(t)+' '\n",
        "    yield s.strip()\n",
        "    s = ''\n",
        "\n",
        "\n",
        "# print('='*20)\n",
        "# for np in iter_nps(doc):\n",
        "#    print(np)\n",
        "#    for t in np.subtree:\n",
        "#      print(f'\\t{t}')\n",
        "\n",
        "# print('='*20)\n",
        "# for np in iter_nps_str(doc):\n",
        "#   print(np)\n",
        "\n",
        "\n",
        "# print('='*20)\n",
        "# for token in doc:\n",
        "#   print(f'{token.text} {token.tag_} {token.dep_} {str(token.dep)} \\t\\t\\thead: {token.head.tag_} {token.head.dep_} {token.head.text}')     \n",
        "#   for t in token.subtree:\n",
        "#     print(f'\\t{t}')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rqmuFxy0Ylpa",
        "colab_type": "text"
      },
      "source": [
        "##  Set up Tensorflow graph"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AV9S-wKeWG8A",
        "colab_type": "code",
        "outputId": "48133f13-8a19-43b6-dd1b-640cc0dfc8cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 604
        }
      },
      "source": [
        "%%time\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import numpy as np\n",
        "import tf_sentencepiece\n",
        "\n",
        "# Set up graph.\n",
        "g = tf.Graph()\n",
        "with g.as_default():\n",
        "  questions_input = tf.placeholder(dtype=tf.string, shape=[None])\n",
        "  responses_input = tf.placeholder(dtype=tf.string, shape=[None])\n",
        "  contexts_input = tf.placeholder(dtype=tf.string, shape=[None])\n",
        "\n",
        "\n",
        "  module = hub.Module(\"https://tfhub.dev/google/universal-sentence-encoder-multilingual-qa/1\")\n",
        "  question_embeddings = module(\n",
        "    dict(input=questions_input),\n",
        "    signature=\"question_encoder\", as_dict=True)\n",
        "\n",
        "  response_embeddings = module(\n",
        "    dict(input=responses_input,\n",
        "         context=contexts_input),\n",
        "    signature=\"response_encoder\", as_dict=True)\n",
        "\n",
        "  init_op = tf.group([tf.global_variables_initializer(), tf.tables_initializer()])\n",
        "g.finalize()\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 35.6 s, sys: 1.73 s, total: 37.3 s\n",
            "Wall time: 40.1 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jdHhNc-_-1Jn",
        "colab_type": "text"
      },
      "source": [
        "# Run"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EXQYvsUn-6g2",
        "colab_type": "text"
      },
      "source": [
        "##Initialize tensorflow session."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zqSZ5-4KtDTp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "ab6995d5-2c48-4402-b37c-724ac06a4da3"
      },
      "source": [
        "%%time\n",
        "# Initialize session.\n",
        "session = tf.Session(graph=g)\n",
        "session.run(init_op)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 10.5 s, sys: 1.96 s, total: 12.5 s\n",
            "Wall time: 14.7 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gWWWJ_LPC9Hw",
        "colab_type": "text"
      },
      "source": [
        "## Tests"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pu_3kKA8ZpXC",
        "colab_type": "code",
        "outputId": "ceb02818-04e2-41b4-fdb5-f4f07ddef773",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# %%time\n",
        "sentences = '''\n",
        "F.\tPROGRAM LIMITS OF LIABILITY\n",
        "\n",
        "(1)\tThe Program limit of liability is US$400,000,000. \n",
        "\n",
        "(2)\tSublimits below are applicable to all direct physical loss, damage or destruction insured against, except an Accident. \n",
        "\n",
        "(a)\tWith respect to all losses caused by the peril of Flood, the Company shall not be liable, in the aggregate for any one Policy year, for more than its proportionate share of US$25,000,000. \n",
        "\n",
        "But not to exceed US$5,000,000 in the aggregate for any one Policy year\n",
        "for the peril of Flood occurring in Special Flood Hazard Areas. However,\n",
        "this Special Flood Hazard Area sublimit shall not apply to loss involving \n",
        "personal property, buildings, or structures wholly located outside an area\n",
        "designated as a Special Flood Hazard Area.\n",
        "\n",
        "The aggregate limit stated in the paragraph above shall be part of the overall aggregate limit stated in the introduction to this paragraph.\n",
        "\n",
        "Even if the peril of Flood is the predominant cause of direct physical loss, damage or destruction, any ensuing physical loss, damage or destruction arising from a peril not otherwise excluded herein shall not be subject to the sublimit or aggregate specified in this paragraph (a).\n",
        "'''\n",
        "sentence1 = 'With respect to all losses caused by the peril of Earthquake, the Company shall not be liable, in the aggregate for any one Policy year, for more than its proportionate share of US$50,000,000.'\n",
        "\n",
        "sentence2 = '''\n",
        "Суд освободил из-под стражи следователя Алексея Шиманского и полицейского Кирилла Лисового из Санкт-Петербурга, которых подозревают в посредничестве при передаче взятки в 19 миллионов рублей. Об этом сообщает «Фонтанка».\n",
        "'''\n",
        "\n",
        "import simpleneighbors\n",
        "\n",
        "def  qa(sentences, queries, use_parser = True):\n",
        "  index = simpleneighbors.SimpleNeighbors(\n",
        "      512, metric='angular')\n",
        "\n",
        "  def candidate_generation(t, from_n=1, to_n=6):\n",
        "    l = t.split()\n",
        "    grams = []\n",
        "    for n in range(from_n, to_n):\n",
        "      grams += [' '.join(l[i:i+n]) for i in range(len(l)-n+1)]\n",
        "    \n",
        "    return grams\n",
        "\n",
        "\n",
        "  def display_nearest_neighbors(query_text):\n",
        "    print(f'Query: {query_text}')\n",
        "    query_embedding = session.run(question_embeddings, feed_dict={questions_input: [query_text]})['outputs'][0]\n",
        "    # print(\"query_embedding:\")\n",
        "    # print(query_embedding)\n",
        "    search_results = index.nearest(query_embedding, n=10)\n",
        "    print('Top-10 Responses:')\n",
        "    for s in search_results:\n",
        "      print(f'\\t{s}')\n",
        "\n",
        "\n",
        "  def calculate_sentences_emb(sentences, use_parser=True):\n",
        "    responses = []\n",
        "    contexts = []\n",
        "    if use_parser:\n",
        "      doc = nlp(sentences)\n",
        "      for sent in doc.sents:\n",
        "        # print('='*40)\n",
        "        # print(sent)\n",
        "        # print('-- Noun groups & attributes: --')\n",
        "        for np in iter_nps_str(sent):\n",
        "          # print(f'\\t{np}')\n",
        "          responses.append(str(np).strip())\n",
        "          contexts.append(str(sent).strip())\n",
        "    else:\n",
        "      grams = candidate_generation(sentences, 1, 5)\n",
        "      for g in grams:\n",
        "        responses.append(g.strip())\n",
        "        contexts.append(sentences.strip())\n",
        "\n",
        "\n",
        "    candidate_embeddings = session.run(\n",
        "        response_embeddings,\n",
        "        feed_dict={\n",
        "            responses_input: responses,\n",
        "            contexts_input: contexts\n",
        "        })\n",
        "    print(f'Shape: {candidate_embeddings[\"outputs\"].shape}')\n",
        "    # print(candidate_embeddings[\"outputs\"][0])\n",
        "    print('Candidates:')\n",
        "    for i in range(len(responses)):\n",
        "      print(f'\\t{responses[i]}')\n",
        "      index.add_one(responses[i], candidate_embeddings['outputs'][i])\n",
        "\n",
        "    index.build()\n",
        "    \n",
        "  calculate_sentences_emb(sentences, use_parser)\n",
        "  # display_nearest_neighbors('Какие имена у освобождённых')\n",
        "  if isinstance(queries, list):\n",
        "    for q in queries:\n",
        "      display_nearest_neighbors(q)\n",
        "  else:\n",
        "      display_nearest_neighbors(queries)\n",
        "\n",
        "\n",
        "\n",
        "# qa(sentences, 'имена отпущенных')\n",
        "qa(sentence1, ['peril','sum'])\n",
        "qa(sentence2, ['имена отпущенных', 'сумма взятки'])\n",
        "  "
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape: (10, 512)\n",
            "Candidates:\n",
            "\trespect to all losses caused by the peril of Earthquake\n",
            "\tall losses caused by the peril of Earthquake\n",
            "\tthe peril of Earthquake\n",
            "\tEarthquake\n",
            "\tthe Company\n",
            "\tthe aggregate for any one Policy year\n",
            "\tany one Policy year\n",
            "\tmore than its proportionate share of US$ 50,000,000\n",
            "\tits proportionate share of US$ 50,000,000\n",
            "\tUS$ 50,000,000\n",
            "Query: peril\n",
            "Top-10 Responses:\n",
            "\tthe peril of Earthquake\n",
            "\trespect to all losses caused by the peril of Earthquake\n",
            "\tall losses caused by the peril of Earthquake\n",
            "\tthe Company\n",
            "\tEarthquake\n",
            "\tany one Policy year\n",
            "\tUS$ 50,000,000\n",
            "\tmore than its proportionate share of US$ 50,000,000\n",
            "\tthe aggregate for any one Policy year\n",
            "\tits proportionate share of US$ 50,000,000\n",
            "Query: sum\n",
            "Top-10 Responses:\n",
            "\tmore than its proportionate share of US$ 50,000,000\n",
            "\tits proportionate share of US$ 50,000,000\n",
            "\tthe aggregate for any one Policy year\n",
            "\tUS$ 50,000,000\n",
            "\tEarthquake\n",
            "\tthe Company\n",
            "\trespect to all losses caused by the peril of Earthquake\n",
            "\tthe peril of Earthquake\n",
            "\tall losses caused by the peril of Earthquake\n",
            "\tany one Policy year\n",
            "Shape: (8, 512)\n",
            "Candidates:\n",
            "\tСуд\n",
            "\tиз - под стражи\n",
            "\tАлексея Шиманского\n",
            "\tКирилла Лисового из Санкт - Петербурга ,\n",
            "\tкоторых\n",
            "\tпосредничестве\n",
            "\tпри передаче\n",
            "\tв 19 миллионов рублей\n",
            "Query: имена отпущенных\n",
            "Top-10 Responses:\n",
            "\tАлексея Шиманского\n",
            "\tКирилла Лисового из Санкт - Петербурга ,\n",
            "\tкоторых\n",
            "\tСуд\n",
            "\tпри передаче\n",
            "\tпосредничестве\n",
            "\tиз - под стражи\n",
            "\tв 19 миллионов рублей\n",
            "Query: сумма взятки\n",
            "Top-10 Responses:\n",
            "\tв 19 миллионов рублей\n",
            "\tСуд\n",
            "\tкоторых\n",
            "\tпосредничестве\n",
            "\tпри передаче\n",
            "\tиз - под стражи\n",
            "\tАлексея Шиманского\n",
            "\tКирилла Лисового из Санкт - Петербурга ,\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ctGvm-GsFPW7",
        "colab_type": "text"
      },
      "source": [
        "# Form"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4V2drB5rE1TJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "cellView": "both",
        "outputId": "9d2b5049-39df-421f-ed76-bbddf55eef79"
      },
      "source": [
        "%%time\n",
        "\n",
        "#@title Question and answer\n",
        "text = \"Deductable $100,000 per occurrence - Earthquake.\" #@param {type:\"string\"}\n",
        "query = \"deductable\" #@param {type:\"string\"}\n",
        "use_token_ngram = False #@param {type:\"boolean\"}\n",
        "\n",
        "qa(text, [query], not use_token_ngram)\n"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape: (1, 512)\n",
            "Candidates:\n",
            "\toccurrence - Earthquake\n",
            "Query: deductable\n",
            "Top-10 Responses:\n",
            "\toccurrence - Earthquake\n",
            "CPU times: user 69 ms, sys: 7.2 ms, total: 76.2 ms\n",
            "Wall time: 76.3 ms\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}