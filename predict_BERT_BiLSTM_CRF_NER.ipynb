{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "predict_BERT-BiLSTM-CRF-NER.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thingumajig/colab-experiments/blob/master/predict_BERT_BiLSTM_CRF_NER.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cBYIXlQrLd4L",
        "colab_type": "text"
      },
      "source": [
        "#Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_plzSmULk1s",
        "colab_type": "text"
      },
      "source": [
        "## clone from git"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DLCUQARCnTvN",
        "colab_type": "code",
        "outputId": "0bc009eb-ad4a-49b7-9246-497d06f66081",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        }
      },
      "source": [
        "#!pip install --upgrade tensorflow-gpu\n",
        "#!pip install seqeval\n",
        "!pip install bert-tensorflow\n",
        "\n",
        "!pip list | grep tensorflow\n",
        "#!ps ax | grep python\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting bert-tensorflow\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/66/7eb4e8b6ea35b7cc54c322c816f976167a43019750279a8473d355800a93/bert_tensorflow-1.0.1-py2.py3-none-any.whl (67kB)\n",
            "\r\u001b[K     |████▉                           | 10kB 15.6MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 20kB 3.2MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 30kB 4.7MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 40kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 51kB 3.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 61kB 4.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 71kB 4.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from bert-tensorflow) (1.12.0)\n",
            "Installing collected packages: bert-tensorflow\n",
            "Successfully installed bert-tensorflow-1.0.1\n",
            "bert-tensorflow          1.0.1                \n",
            "mesh-tensorflow          0.0.5                \n",
            "tensorflow               1.14.0               \n",
            "tensorflow-estimator     1.14.0               \n",
            "tensorflow-hub           0.5.0                \n",
            "tensorflow-metadata      0.14.0               \n",
            "tensorflow-probability   0.7.0                \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JetNC4E8ymyy",
        "colab_type": "code",
        "outputId": "5ff0e337-31e8-4390-c903-2020a87df6ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "\n",
        "import sys\n",
        "\n",
        "# bert_path = 'bert'\n",
        "# !test -d $bert_path || git clone https://github.com/google-research/bert $bert_path\n",
        "# if not bert_path in sys.path:\n",
        "#   sys.path.insert(0, bert_path)\n",
        "\n",
        "\n",
        "\n",
        "bert_ner_path = 'bert-ner'  \n",
        "!test -d $bert_ner_path || git clone https://github.com/thingumajig/colab-experiments $bert_ner_path\n",
        "\n",
        "if not bert_ner_path in sys.path:\n",
        "  sys.path.insert(0, bert_ner_path)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'bert-ner'...\n",
            "remote: Enumerating objects: 79, done.\u001b[K\n",
            "remote: Counting objects: 100% (79/79), done.\u001b[K\n",
            "remote: Compressing objects: 100% (69/69), done.\u001b[K\n",
            "remote: Total 79 (delta 39), reused 24 (delta 8), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (79/79), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H93ZJ86sHeeF",
        "colab_type": "code",
        "outputId": "0539c7be-0c5a-4b85-ef92-9c9e48ee8007",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "#!rm -rfv $bert_ner_path/bert\n",
        "!git -C $bert_ner_path pull\n",
        "if not bert_ner_path in sys.path:\n",
        "  sys.path.insert(0, bert_ner_path)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Already up to date.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nl1oHKK1LvuA",
        "colab_type": "text"
      },
      "source": [
        "##Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z0o6FSSvy2yW",
        "colab_type": "code",
        "outputId": "94e35541-9f4a-4da1-8f37-a17fabdba5f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        }
      },
      "source": [
        "#@title Select model:\n",
        "import os\n",
        "\n",
        "bert_model = \"https://storage.googleapis.com/bert_models/2019_05_30/wwm_cased_L-24_H-1024_A-16.zip\" #@param [\"https://storage.googleapis.com/bert_models/2019_05_30/wwm_cased_L-24_H-1024_A-16.zip\", \"https://storage.googleapis.com/bert_models/2018_11_23/multi_cased_L-12_H-768_A-12.zip\"]\n",
        "bert_model_zip = os.path.basename(bert_model)\n",
        "bert_model_dir = os.path.splitext(bert_model_zip)[0]\n",
        "\n",
        "!test -d $bert_model_dir || wget $bert_model\n",
        "\n",
        "\n",
        "!test -d $bert_model_dir || unzip /content/$bert_model_zip"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-07-26 19:08:44--  https://storage.googleapis.com/bert_models/2019_05_30/wwm_cased_L-24_H-1024_A-16.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.197.128, 2607:f8b0:400e:c07::80\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.197.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1242589256 (1.2G) [application/zip]\n",
            "Saving to: ‘wwm_cased_L-24_H-1024_A-16.zip’\n",
            "\n",
            "wwm_cased_L-24_H-10 100%[===================>]   1.16G   114MB/s    in 9.2s    \n",
            "\n",
            "2019-07-26 19:08:54 (128 MB/s) - ‘wwm_cased_L-24_H-1024_A-16.zip’ saved [1242589256/1242589256]\n",
            "\n",
            "Archive:  /content/wwm_cased_L-24_H-1024_A-16.zip\n",
            "   creating: wwm_cased_L-24_H-1024_A-16/\n",
            "  inflating: wwm_cased_L-24_H-1024_A-16/bert_model.ckpt.meta  \n",
            "  inflating: wwm_cased_L-24_H-1024_A-16/bert_model.ckpt.data-00000-of-00001  \n",
            "  inflating: wwm_cased_L-24_H-1024_A-16/vocab.txt  \n",
            "  inflating: wwm_cased_L-24_H-1024_A-16/bert_model.ckpt.index  \n",
            "  inflating: wwm_cased_L-24_H-1024_A-16/bert_config.json  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TSVMaPtiIO_t",
        "colab_type": "code",
        "outputId": "4f4b404e-a9c5-4de0-a313-f44ffd926f8d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCGDQ0tbLVGR",
        "colab_type": "text"
      },
      "source": [
        "##init flags"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9KCSUfYEIjOP",
        "colab_type": "code",
        "outputId": "b99a5b9d-1c0e-4707-bd68-35acb94f5d40",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import collections\n",
        "import os\n",
        "import json\n",
        "\n",
        "import tensorflow as tf\n",
        "import codecs\n",
        "\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "\n",
        "flags = tf.flags\n",
        "FLAGS = flags.FLAGS\n",
        "\n",
        "# print(FLAGS.__dict__['__wrapped'])\n",
        "def del_all_flags(FLAGS, r = []):\n",
        "    flags_dict = FLAGS._flags()\n",
        "    keys_list = [keys for keys in flags_dict]\n",
        "    for keys in keys_list:\n",
        "        if keys in r:\n",
        "          FLAGS.__delattr__(keys)\n",
        " \n",
        "# allows rerun cell!\n",
        "del_all_flags(FLAGS, r = ['bert_config_file','data_dir',\n",
        "                         \"task_name\", \"output_dir\",\"init_checkpoint\",\n",
        "                         \"do_lower_case\", \"do_train\", \"use_tpu\", \"do_predict\",\n",
        "                         \"use_crf\", \"train_batch_size\", \"eval_batch_size\",\n",
        "                         \"learning_rate\", \"num_train_epochs\", \"warmup_proportion\",\n",
        "                         \"bert_dropout_rate\", \"bilstm_dropout_rate\",\n",
        "                          \"save_checkpoints_steps\", \"save_summary_steps\",\n",
        "                         'data_config_path', \"num_tpu_cores\", \"master\",\n",
        "                          \"vocab_file\", \"keep_checkpoint_max\",                          \n",
        "                          'lstm_size', 'max_seq_length',\n",
        "                          'predict_batch_size', 'iterations_per_loop',\n",
        "                          'f',\n",
        "                          \n",
        "                         ])\n",
        "\n",
        "train_model_path = '/content/drive/My Drive/ner_checkpoints'\n",
        "if not os.path.exists(train_model_path):\n",
        "    os.makedirs(train_model_path)\n",
        "\n",
        "flags.DEFINE_string(\"data_dir\", f'/content/{bert_ner_path}/data/CoNLL', \"The input datadir. ex) 'NERdata'\")\n",
        "flags.DEFINE_string(\"bert_config_file\", os.path.join('/content',bert_model_dir,'bert_config.json'), \"The config json file corresponding to the pre-trained BERT model. ex) 'bert_config.json'\")\n",
        "\n",
        "flags.DEFINE_string(\"task_name\", 'NER', \"The name of the task to train. ex) 'NER'\")\n",
        "\n",
        "flags.DEFINE_string(\"output_dir\", train_model_path, \"The output directory where the model checkpoints will be written. ex) 'output'\")\n",
        "\n",
        "flags.DEFINE_string(\"init_checkpoint\", f\"/content/{bert_model_dir}/bert_model.ckpt\", \"Initial checkpoint (usually from a pre-trained BERT model).\")\n",
        "\n",
        "flags.DEFINE_bool(\"do_lower_case\", False, \"Whether to lower case the input text.\")\n",
        "\n",
        "flags.DEFINE_integer(\"max_seq_length\", 128, \"The maximum total input sequence length after WordPiece tokenization.\")\n",
        "\n",
        "flags.DEFINE_bool(\"do_train\", False, \"Whether to run training.\")\n",
        "flags.DEFINE_bool(\"do_predict\", True, \"Whether to run the model in inference mode on the test set.\")\n",
        "\n",
        "flags.DEFINE_bool(\"use_crf\", True, \"Whether to use CRF decoding.\")\n",
        "\n",
        "flags.DEFINE_integer(\"train_batch_size\", 16, \"Total batch size for training.\")\n",
        "flags.DEFINE_integer(\"eval_batch_size\", 8, \"Total batch size for eval.\")\n",
        "flags.DEFINE_integer(\"predict_batch_size\", 8, \"Total batch size for predict.\")\n",
        "flags.DEFINE_float(\"learning_rate\", 5e-5, \"The initial learning rate for Adam.\")\n",
        "\n",
        "flags.DEFINE_float(\"num_train_epochs\", 3.0, \"Total number of training epochs to perform.\")\n",
        "flags.DEFINE_float(\"warmup_proportion\", 0.1, \"Proportion of training to perform linear learning rate warmup for. E.g., 0.1 = 10% of training.\")\n",
        "flags.DEFINE_float(\"bert_dropout_rate\", 0.2, \"Proportion of dropout for bert embedding.\")\n",
        "flags.DEFINE_float(\"bilstm_dropout_rate\", 0.2, \"Proportion of dropout for bilstm.\")\n",
        "flags.DEFINE_integer(\"save_checkpoints_steps\", 1000, \"How often to save the model checkpoint.\")\n",
        "flags.DEFINE_integer(\"save_summary_steps\", 100, \"Save summaries every this many steps\")\n",
        "flags.DEFINE_integer(\"keep_checkpoint_max\", 20, \"The maximum number of recent checkpoint files to keep\")\n",
        "flags.DEFINE_integer(\"iterations_per_loop\", 1000, \"How many steps to make in each estimator call.\")\n",
        "\n",
        "flags.DEFINE_string(\"vocab_file\", f'/content/{bert_model_dir}/vocab.txt', \"The vocabulary file that the BERT model was trained on. ex) 'vocab.txt'\")\n",
        "\n",
        "flags.DEFINE_string(\"master\", None, \"[Optional] TensorFlow master URL.\")\n",
        "\n",
        "flags.DEFINE_bool(\"use_tpu\", False, \"Whether to use TPU or GPU/CPU.\")\n",
        "flags.DEFINE_integer( \"num_tpu_cores\", 8, \"Only used if `use_tpu` is True. Total number of TPU cores to use.\")\n",
        "\n",
        "flags.DEFINE_string('data_config_path', 'data.conf', 'data config file, which save train and dev config')\n",
        "\n",
        "flags.DEFINE_integer('lstm_size', 128, 'size of lstm units')\n",
        "\n",
        "flags.DEFINE_string(\"f\", 'f', \"f\") #????? error otherwise\n",
        "\n",
        "print(FLAGS.__dict__['__wrapped'])\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:\n",
            "  --bert_config_file: The config json file corresponding to the pre-trained BERT\n",
            "    model. ex) 'bert_config.json'\n",
            "    (default: '/content/wwm_cased_L-24_H-1024_A-16/bert_config.json')\n",
            "  --bert_dropout_rate: Proportion of dropout for bert embedding.\n",
            "    (default: '0.2')\n",
            "    (a number)\n",
            "  --bilstm_dropout_rate: Proportion of dropout for bilstm.\n",
            "    (default: '0.2')\n",
            "    (a number)\n",
            "  --data_config_path: data config file, which save train and dev config\n",
            "    (default: 'data.conf')\n",
            "  --data_dir: The input datadir. ex) 'NERdata'\n",
            "    (default: '/content/bert-ner/data/CoNLL')\n",
            "  --[no]do_lower_case: Whether to lower case the input text.\n",
            "    (default: 'false')\n",
            "  --[no]do_predict: Whether to run the model in inference mode on the test set.\n",
            "    (default: 'true')\n",
            "  --[no]do_train: Whether to run training.\n",
            "    (default: 'false')\n",
            "  --eval_batch_size: Total batch size for eval.\n",
            "    (default: '8')\n",
            "    (an integer)\n",
            "  --init_checkpoint: Initial checkpoint (usually from a pre-trained BERT model).\n",
            "    (default: '/content/wwm_cased_L-24_H-1024_A-16/bert_model.ckpt')\n",
            "  --iterations_per_loop: How many steps to make in each estimator call.\n",
            "    (default: '1000')\n",
            "    (an integer)\n",
            "  --keep_checkpoint_max: The maximum number of recent checkpoint files to keep\n",
            "    (default: '20')\n",
            "    (an integer)\n",
            "  --learning_rate: The initial learning rate for Adam.\n",
            "    (default: '5e-05')\n",
            "    (a number)\n",
            "  --lstm_size: size of lstm units\n",
            "    (default: '128')\n",
            "    (an integer)\n",
            "  --master: [Optional] TensorFlow master URL.\n",
            "  --max_seq_length: The maximum total input sequence length after WordPiece\n",
            "    tokenization.\n",
            "    (default: '128')\n",
            "    (an integer)\n",
            "  --num_tpu_cores: Only used if `use_tpu` is True. Total number of TPU cores to\n",
            "    use.\n",
            "    (default: '8')\n",
            "    (an integer)\n",
            "  --num_train_epochs: Total number of training epochs to perform.\n",
            "    (default: '3.0')\n",
            "    (a number)\n",
            "  --output_dir: The output directory where the model checkpoints will be\n",
            "    written. ex) 'output'\n",
            "    (default: '/content/drive/My Drive/ner_checkpoints')\n",
            "  --predict_batch_size: Total batch size for predict.\n",
            "    (default: '8')\n",
            "    (an integer)\n",
            "  --save_checkpoints_steps: How often to save the model checkpoint.\n",
            "    (default: '1000')\n",
            "    (an integer)\n",
            "  --save_summary_steps: Save summaries every this many steps\n",
            "    (default: '100')\n",
            "    (an integer)\n",
            "  --task_name: The name of the task to train. ex) 'NER'\n",
            "    (default: 'NER')\n",
            "  --train_batch_size: Total batch size for training.\n",
            "    (default: '16')\n",
            "    (an integer)\n",
            "  --[no]use_crf: Whether to use CRF decoding.\n",
            "    (default: 'true')\n",
            "  --[no]use_tpu: Whether to use TPU or GPU/CPU.\n",
            "    (default: 'false')\n",
            "  --vocab_file: The vocabulary file that the BERT model was trained on. ex)\n",
            "    'vocab.txt'\n",
            "    (default: '/content/wwm_cased_L-24_H-1024_A-16/vocab.txt')\n",
            "  --warmup_proportion: Proportion of training to perform linear learning rate\n",
            "    warmup for. E.g., 0.1 = 10% of training.\n",
            "    (default: '0.1')\n",
            "    (a number)\n",
            "\n",
            "absl.app:\n",
            "  --[no]only_check_args: Set to true to validate args and exit.\n",
            "    (default: 'false')\n",
            "  --[no]pdb_post_mortem: Set to true to handle uncaught exceptions with PDB post\n",
            "    mortem.\n",
            "    (default: 'false')\n",
            "  --profile_file: Dump profile information to a file (for python -m pstats).\n",
            "    Implies --run_with_profiling.\n",
            "  --[no]run_with_pdb: Set to true for PDB debug mode\n",
            "    (default: 'false')\n",
            "  --[no]run_with_profiling: Set to true for profiling the script. Execution will\n",
            "    be slower, and the output format might change over time.\n",
            "    (default: 'false')\n",
            "  --[no]use_cprofile_for_profiling: Use cProfile instead of the profile module\n",
            "    for profiling. This has no effect unless --run_with_profiling is set.\n",
            "    (default: 'true')\n",
            "\n",
            "absl.logging:\n",
            "  --[no]alsologtostderr: also log to stderr?\n",
            "    (default: 'false')\n",
            "  --log_dir: directory to write logfiles into\n",
            "    (default: '')\n",
            "  --[no]logtostderr: Should only log to stderr?\n",
            "    (default: 'false')\n",
            "  --[no]showprefixforinfo: If False, do not prepend prefix to info messages when\n",
            "    it's logged to stderr, --verbosity is set to INFO level, and python logging\n",
            "    is used.\n",
            "    (default: 'true')\n",
            "  --stderrthreshold: log messages at this level, or more severe, to stderr in\n",
            "    addition to the logfile.  Possible values are 'debug', 'info', 'warning',\n",
            "    'error', and 'fatal'.  Obsoletes --alsologtostderr. Using --alsologtostderr\n",
            "    cancels the effect of this flag. Please also note that this flag is subject\n",
            "    to --verbosity and requires logfile not be stderr.\n",
            "    (default: 'fatal')\n",
            "  -v,--verbosity: Logging verbosity level. Messages logged at this level or\n",
            "    lower will be included. Set to 1 for debug logging. If the flag was not set\n",
            "    or supplied, the value will be changed from the default of -1 (warning) to 0\n",
            "    (info) after flags are parsed.\n",
            "    (default: '-1')\n",
            "    (an integer)\n",
            "\n",
            "absl.testing.absltest:\n",
            "  --test_random_seed: Random seed for testing. Some test frameworks may change\n",
            "    the default value of this flag between runs, so it is not appropriate for\n",
            "    seeding probabilistic tests.\n",
            "    (default: '301')\n",
            "    (an integer)\n",
            "  --test_randomize_ordering_seed: If positive, use this as a seed to randomize\n",
            "    the execution order for test cases. If \"random\", pick a random seed to use.\n",
            "    If 0 or not set, do not randomize test case execution order. This flag also\n",
            "    overrides the TEST_RANDOMIZE_ORDERING_SEED environment variable.\n",
            "  --test_srcdir: Root of directory tree where source files live\n",
            "    (default: '')\n",
            "  --test_tmpdir: Directory for temporary testing files\n",
            "    (default: '/tmp/absl_testing')\n",
            "  --xml_output_file: File to store XML test results\n",
            "    (default: '')\n",
            "\n",
            "tensorflow.python.ops.parallel_for.pfor:\n",
            "  --[no]op_conversion_fallback_to_while_loop: If true, falls back to using a\n",
            "    while loop for ops for which a converter is not defined.\n",
            "    (default: 'false')\n",
            "\n",
            "absl.flags:\n",
            "  --flagfile: Insert flag definitions from the given file into the command line.\n",
            "    (default: '')\n",
            "  --undefok: comma-separated list of flag names that it is okay to specify on\n",
            "    the command line even if the program does not define a flag with that name.\n",
            "    IMPORTANT: flags in this list that have arguments MUST use the --flag=value\n",
            "    format.\n",
            "    (default: '')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wsCTt3R5EiaA",
        "colab_type": "text"
      },
      "source": [
        "# run"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JATuhs7ngX4O",
        "colab_type": "code",
        "outputId": "38daa979-4f1a-4d67-92c3-247c16b6ff01",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "import bert_bilstm_model as m\n",
        "from estimator_train_predict import *\n",
        "\n",
        "processors = {\n",
        "    \"ner\": m.NerProcessor\n",
        "}\n",
        "\n",
        "task_name = FLAGS.task_name.lower()\n",
        "if task_name not in processors:\n",
        "    raise ValueError(\"Task not found: %s\" % (task_name))\n",
        "processor = processors[task_name]()\n",
        "label_list = processor.get_labels()\n",
        "\n",
        "\n",
        "estimator, tokenizer = create_estimator(label_list)\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0726 19:09:36.467666 139861836302208 estimator.py:1984] Estimator's model_fn (<function model_fn_builder.<locals>.model_fn at 0x7f33c02f0ae8>) includes params argument, but params are not passed to Estimator.\n",
            "W0726 19:09:36.469970 139861836302208 tpu_context.py:211] eval_on_tpu ignored because use_tpu is False.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "================== use WarmStartSettings from /content/drive/My Drive/ner_checkpoints\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wswJn_eZEkbI",
        "colab_type": "code",
        "outputId": "480dcd54-557c-495b-f250-23b2e252e280",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "text = '''\n",
        "The Puerto Rican Legislature gave Gov. Ricardo Rosselló a choice by saying that they're ready to start impeachment proceedings unless he resigns first.\n",
        "\n",
        "The president of Puerto Rico’s House of Representatives, Carlos Méndez Núñez, has called for an extraordinary legislative session to officially begin an impeachment process against Rosselló Thursday afternoon.\n",
        "The news came on Wednesday after three attorneys, commissioned by Méndez Núñez, unanimously determined there were five offenses, stemming from leaked scandalous chats, that constitute grounds for impeachment.\n",
        "\n",
        "In the report that was leaked to the press, the attorneys found Rosselló committed four serious offenses and one misdemeanor, including illicitly using public resources and services for partisan purposes, as well as allowing government officials and contractors to misuse public funds and time for non-government work.\n",
        "\n",
        "During a press conference, Méndez Núñez said the only thing that would stop the process is if the governor resigns.\n",
        "\n",
        "If Rosselló steps down, then the report would be passed on to the relevant authorities, he added.\n",
        "\n",
        "On Wednesday evening, a crowd of thousands had gathered in Old San Juan near the governor's mansion, shouting \"Ricky, Renuncia!\"\n",
        "\n",
        "One of the protesters, Adrián, told NBC News that \"tonight was supposed to be a dance party.\" But since Rosselló has consistently refused to resign, he's afraid things will get ugly.\n",
        "\n",
        "“This will be bad,” said Adrián, who brought his gas mask and goggles as a precaution measure.\n",
        "\n",
        "On Wednesday evening, a large number of reporters were assembled for hours at the door of the governor's mansion, waiting for what was expected to be a much earlier announcement from Rosselló or his aides about his future.\n",
        "\n",
        "Rosselló's public affairs secretary, Anthony Maceira, addressed reporters two hours later, but only said that the governor was working on a statement that he would give at an unspecified time.\n",
        "\n",
        "The turmoil follows the island's largest protest in recent history calling for Rosselló's ouster over scandals involving leaked private chats as well as corruption investigations and arrests.\n",
        "\n",
        "News of Rosselló's impeachment process came a day after NBC News and Telemundo, both owned by NBC Universal, reported that the island's Justice Department had issued search warrants to confiscate the cellphones of several people who took part in the private chats.\n",
        "\n",
        "Hundreds of thousands of Puerto Ricans have been protesting for 12 consecutive days, demanding Rosselló's ouster. Protests continued to grow on the island after Rosselló announced on Sunday that he wouldn't run for re-election and that he would step down from the leadership of the pro-statehood New Progressive Party.\n",
        "\n",
        "On Monday, more than a half-million people paralyzed metropolitan San Juan in protest, marching across one of the main highways despite heavy rain in a \"March of the People\" that ended late in the night as police fired tear gas canisters.\n",
        "'''\n",
        "\n",
        "text1 = '''\n",
        "Republicans in the Senate have twice in 24 hours blocked the advancement of bills aimed at strengthening election security just hours after former special counsel Robert Mueller warned of the continued threat that foreign powers interfering in US elections.\n",
        "\n",
        "Senate Majority Leader Mitch McConnell came to the Senate floor Thursday to personally object to House-passed legislation backed by Democrats. This comes after Republican Sen. Cindy Hyde-Smith of Mississippi objected to a trio of bills on Wednesday, in keeping with long standing GOP arguments that Congress has already responded to election security needs for the upcoming election.\n",
        "Democratic Sens. Mark Warner of Virginia, Richard Blumenthal of Connecticut and Ron Wyden of Oregon had advocated for the bills on the Senate floor, asking for unanimous consent to pass the package, but that ask can be halted with an objection from any senator.\n",
        "Two of those bills would require campaigns to report to federal authorities any attempts by foreign entities to interfere in US elections, and the third is aimed at protecting from hackers the personal accounts and devices of senators and some staffers.\n",
        "On Thursday, McConnell called the House-passed legislation Democrats wanted to clear \"so partisan it received just one Republican vote over in the House.\" He added that any election security legislation the Senate takes up must be bipartisan and also alleged that this bill is being pushed by the same Democrats who pushed the \"conspiracy theory\" of President Donald Trump and Russia.\n",
        "'''\n",
        "\n",
        "import pickle\n",
        "with codecs.open(os.path.join(FLAGS.output_dir, 'label2id.pkl'), 'rb') as rf:\n",
        "  label2id = pickle.load(rf)\n",
        "  id2label = {value: key for key, value in label2id.items()}\n",
        "\n",
        "for (id, lbl) in id2label.items():\n",
        "  print(f'{id}={lbl}')\n",
        "\n",
        "predict_examples, result = predict(processor, estimator, tokenizer, label_list, text)\n",
        "result = list(result)\n",
        "\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0726 19:10:36.120021 139861836302208 deprecation_wrapper.py:119] From bert-ner/bert_bilstm_model.py:298: The name tf.python_io.TFRecordWriter is deprecated. Please use tf.io.TFRecordWriter instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1=O\n",
            "2=B-PER\n",
            "3=I-PER\n",
            "4=B-ORG\n",
            "5=I-ORG\n",
            "6=B-LOC\n",
            "7=I-LOC\n",
            "8=B-MISC\n",
            "9=I-MISC\n",
            "10=X\n",
            "================= Number of predict examples:527\n",
            "Writing example 0 of 527\n",
            "*** Example ***\n",
            "guid: predict-0\n",
            "tokens: The\n",
            "input_ids: 101 1109 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "input_mask: 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "label_ids: 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "*** Example ***\n",
            "guid: predict-1\n",
            "tokens: Puerto\n",
            "input_ids: 101 5304 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "input_mask: 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "label_ids: 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "*** Example ***\n",
            "guid: predict-2\n",
            "tokens: Rican\n",
            "input_ids: 101 14028 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "input_mask: 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "label_ids: 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "*** Example ***\n",
            "guid: predict-3\n",
            "tokens: Legislature\n",
            "input_ids: 101 10461 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "input_mask: 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "label_ids: 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "*** Example ***\n",
            "guid: predict-4\n",
            "tokens: gave\n",
            "input_ids: 101 1522 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "input_mask: 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "label_ids: 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "***** Running prediction*****\n",
            "  Num examples = 527 Batch size = 8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0726 19:10:36.934286 139861836302208 deprecation.py:323] From bert-ner/bert_bilstm_model.py:338: map_and_batch (from tensorflow.contrib.data.python.ops.batching) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.experimental.map_and_batch(...)`.\n",
            "W0726 19:10:36.935658 139861836302208 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/data/python/ops/batching.py:273: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation.\n",
            "W0726 19:10:36.938788 139861836302208 deprecation_wrapper.py:119] From bert-ner/bert_bilstm_model.py:318: The name tf.parse_single_example is deprecated. Please use tf.io.parse_single_example instead.\n",
            "\n",
            "W0726 19:10:36.954749 139861836302208 deprecation.py:323] From bert-ner/bert_bilstm_model.py:323: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "W0726 19:10:36.985889 139861836302208 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/bert/modeling.py:171: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "W0726 19:10:36.989139 139861836302208 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/bert/modeling.py:409: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
            "\n",
            "W0726 19:10:37.028689 139861836302208 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/bert/modeling.py:490: The name tf.assert_less_equal is deprecated. Please use tf.compat.v1.assert_less_equal instead.\n",
            "\n",
            "W0726 19:10:37.106573 139861836302208 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/bert/modeling.py:671: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.dense instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "input_fn: batch_size=8 is_training=False\n",
            "*** Features ***\n",
            "  name = input_ids, shape = (?, 128)\n",
            "  name = input_mask, shape = (?, 128)\n",
            "  name = label_ids, shape = (?, 128)\n",
            "  name = segment_ids, shape = (?, 128)\n",
            "shape of input_ids (?, 128)\n",
            "shape of label_ids (?, 128)\n",
            "estimator mode: infer\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0726 19:10:42.220675 139861836302208 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "seq_length 128\n",
            "lengths Tensor(\"Sum:0\", shape=(?,), dtype=int32)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0726 19:10:43.191802 139861836302208 deprecation.py:323] From bert-ner/bert_bilstm_model.py:384: dropout (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.dropout instead.\n",
            "W0726 19:10:43.440197 139861836302208 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/crf/python/ops/crf.py:213: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
            "W0726 19:10:43.535346 139861836302208 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:2403: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "W0726 19:10:43.774132 139861836302208 deprecation_wrapper.py:119] From bert-ner/bert_bilstm_model.py:499: The name tf.train.get_or_create_global_step is deprecated. Please use tf.compat.v1.train.get_or_create_global_step instead.\n",
            "\n",
            "W0726 19:10:43.775262 139861836302208 deprecation_wrapper.py:119] From bert-ner/bert_bilstm_model.py:501: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
            "\n",
            "W0726 19:10:43.778277 139861836302208 deprecation_wrapper.py:119] From bert-ner/bert_bilstm_model.py:503: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "####################\n",
            "shape of output_layer: (?, 128, 1024)\n",
            "embedding_size:1024\n",
            "seq_length:128\n",
            "shape of logit (?, 128, 11)\n",
            "shape of loss ()\n",
            "shape of per_example_loss (?,)\n",
            "num labels:11\n",
            "####################\n",
            "shape of pred_ids (?, 128)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0726 19:10:45.909057 139861836302208 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TkOldqMYv2yg",
        "colab_type": "code",
        "outputId": "4a88729f-cc82-4f2d-9842-e6e10c3f6495",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 331
        }
      },
      "source": [
        "from render_results import *\n",
        "\n",
        "\n",
        "#    output_predict_file = os.path.join(FLAGS.output_dir, \"pred.txt\")\n",
        "#     write_predictions(predict_examples, result, output_predict_file, id2label)\n",
        "from IPython.display import HTML, display\n",
        "display(HTML(\n",
        "  render_predictions_html_v2(predict_examples, result, id2label)\n",
        "))\n"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "<style> \n",
              ".x1 {\n",
              "\tpadding: .2em .3em;\n",
              "    padding-top: 0.2em;\n",
              "    padding-right: 0.3em;\n",
              "    padding-bottom: 0.2em;\n",
              "    padding-left: 0.3em;\n",
              "    margin: 0 .25em;\n",
              "    margin-top: 0px;\n",
              "    margin-right: 0.25em;\n",
              "    margin-bottom: 0px;\n",
              "    margin-left: 0.25em;\n",
              "    line-height: 1;\n",
              "    display: inline-block;\n",
              "    border-radius: .25em;\n",
              "    border-top-left-radius: 0.25em;\n",
              "    border-top-right-radius: 0.25em;\n",
              "    border-bottom-right-radius: 0.25em;\n",
              "    border-bottom-left-radius: 0.25em;\n",
              "}\n",
              "\n",
              ".x2 {\n",
              "    box-sizing: border-box;\n",
              "    content: attr(data-entity);\n",
              "    font-size: .55em;\n",
              "    line-height: 1;\n",
              "    padding: .35em .35em;\n",
              "    padding-top: 0.35em;\n",
              "    padding-right: 0.35em;\n",
              "    padding-bottom: 0.35em;\n",
              "    padding-left: 0.35em;\n",
              "    border-radius: .35em;\n",
              "    text-transform: uppercase;\n",
              "    display: inline-block;\n",
              "    vertical-align: middle;\n",
              "    margin: 0 0 .15rem .5rem;\n",
              "    margin-top: 0px;\n",
              "    margin-right: 0px;\n",
              "    margin-bottom: 0.15rem;\n",
              "    margin-left: 0.5rem;\n",
              "    background: #fff;\n",
              "    background-image: initial;\n",
              "    background-position-x: initial;\n",
              "    background-position-y: initial;\n",
              "    background-size: initial;\n",
              "    background-repeat-x: initial;\n",
              "    background-repeat-y: initial;\n",
              "    background-attachment: initial;\n",
              "    background-origin: initial;\n",
              "    background-clip: initial;\n",
              "    background-color: rgb(255, 255, 255);\n",
              "    font-weight: 700;\n",
              "}\n",
              "\n",
              "</style>\n",
              "The <mark class=\"x1\" style=\"background-color: rgb(166, 226, 45);\">Puerto <sub class=\"x2\">LOC</sub></mark><mark class=\"x1\" style=\"background-color: rgb(166, 226, 45);\">Rican <sub class=\"x2\">MISC</sub></mark>Legislature  gave  Gov  . <mark class=\"x1\" style=\"background-color: rgb(166, 226, 45);\">Ricardo  Rosselló <sub class=\"x2\">PER</sub></mark>a  choice  by  saying  that  they  're  ready  to  start  impeachment  proceedings  unless  he  resigns  first  .  The  president  of <mark class=\"x1\" style=\"background-color: rgb(166, 226, 45);\">Puerto  Rico <sub class=\"x2\">LOC</sub></mark>’  s  House  of  Representatives  , <mark class=\"x1\" style=\"background-color: rgb(166, 226, 45);\">Carlos  Méndez  Núñez <sub class=\"x2\">PER</sub></mark>,  has  called  for  an  extraordinary  legislative  session  to  officially  begin  an  impeachment  process  against <mark class=\"x1\" style=\"background-color: rgb(166, 226, 45);\">Rosselló <sub class=\"x2\">PER</sub></mark>Thursday  afternoon  .  The  news  came  on  Wednesday  after  three  attorneys  ,  commissioned  by <mark class=\"x1\" style=\"background-color: rgb(166, 226, 45);\">Méndez  Núñez <sub class=\"x2\">PER</sub></mark>,  unanimously  determined  there  were  five  offenses  ,  stemming  from  leaked  scandalous  chats  ,  that  constitute  grounds  for  impeachment  .  In  the  report  that  was  leaked  to  the  press  ,  the  attorneys  found <mark class=\"x1\" style=\"background-color: rgb(166, 226, 45);\">Rosselló <sub class=\"x2\">PER</sub></mark>committed  four  serious  offenses  and  one  misdemeanor  ,  including  illicitly  using  public  resources  and  services  for  partisan  purposes  ,  as  well  as  allowing  government  officials  and  contractors  to  misuse  public  funds  and  time  for  non-government  work  .  During  a  press  conference  , <mark class=\"x1\" style=\"background-color: rgb(166, 226, 45);\">Méndez  Núñez <sub class=\"x2\">PER</sub></mark>said  the  only  thing  that  would  stop  the  process  is  if  the  governor  resigns  .  If <mark class=\"x1\" style=\"background-color: rgb(166, 226, 45);\">Rosselló <sub class=\"x2\">PER</sub></mark>steps  down  ,  then  the  report  would  be  passed  on  to  the  relevant  authorities  ,  he  added  .  On  Wednesday  evening  ,  a  crowd  of  thousands  had  gathered  in  Old <mark class=\"x1\" style=\"background-color: rgb(166, 226, 45);\">San <sub class=\"x2\">LOC</sub></mark><mark class=\"x1\" style=\"background-color: rgb(166, 226, 45);\">Juan <sub class=\"x2\">PER</sub></mark>near  the  governor  's  mansion  ,  shouting  `` <mark class=\"x1\" style=\"background-color: rgb(166, 226, 45);\">Ricky <sub class=\"x2\">PER</sub></mark>, <mark class=\"x1\" style=\"background-color: rgb(166, 226, 45);\">Renuncia <sub class=\"x2\">LOC</sub></mark>!  ''  One  of  the  protesters  , <mark class=\"x1\" style=\"background-color: rgb(166, 226, 45);\">Adrián <sub class=\"x2\">PER</sub></mark>,  told <mark class=\"x1\" style=\"background-color: rgb(166, 226, 45);\">NBC <sub class=\"x2\">ORG</sub></mark>News  that  ``  tonight  was  supposed  to  be  a  dance  party  .  ''  But  since <mark class=\"x1\" style=\"background-color: rgb(166, 226, 45);\">Rosselló <sub class=\"x2\">PER</sub></mark>has  consistently  refused  to  resign  ,  he  's  afraid  things  will  get  ugly  .  “  This  will  be  bad  ,  ”  said <mark class=\"x1\" style=\"background-color: rgb(166, 226, 45);\">Adrián <sub class=\"x2\">PER</sub></mark>,  who  brought  his  gas  mask  and  goggles  as  a  precaution  measure  .  On  Wednesday  evening  ,  a  large  number  of  reporters  were  assembled  for  hours  at  the  door  of  the  governor  's  mansion  ,  waiting  for  what  was  expected  to  be  a  much  earlier  announcement  from <mark class=\"x1\" style=\"background-color: rgb(166, 226, 45);\">Rosselló <sub class=\"x2\">PER</sub></mark>or  his  aides  about  his  future  . <mark class=\"x1\" style=\"background-color: rgb(166, 226, 45);\">Rosselló <sub class=\"x2\">PER</sub></mark>'s  public  affairs  secretary  , <mark class=\"x1\" style=\"background-color: rgb(166, 226, 45);\">Anthony <sub class=\"x2\">PER</sub></mark><mark class=\"x1\" style=\"background-color: rgb(166, 226, 45);\">Maceira <sub class=\"x2\">LOC</sub></mark>,  addressed  reporters  two  hours  later  ,  but  only  said  that  the  governor  was  working  on  a  statement  that  he  would  give  at  an  unspecified  time  .  The  turmoil  follows  the  island  's  largest  protest  in  recent  history  calling  for <mark class=\"x1\" style=\"background-color: rgb(166, 226, 45);\">Rosselló <sub class=\"x2\">PER</sub></mark>'s  ouster  over  scandals  involving  leaked  private  chats  as  well  as  corruption  investigations  and  arrests  .  News  of <mark class=\"x1\" style=\"background-color: rgb(166, 226, 45);\">Rosselló <sub class=\"x2\">PER</sub></mark>'s  impeachment  process  came  a  day  after <mark class=\"x1\" style=\"background-color: rgb(166, 226, 45);\">NBC <sub class=\"x2\">ORG</sub></mark>News  and <mark class=\"x1\" style=\"background-color: rgb(166, 226, 45);\">Telemundo <sub class=\"x2\">ORG</sub></mark>,  both  owned  by <mark class=\"x1\" style=\"background-color: rgb(166, 226, 45);\">NBC <sub class=\"x2\">ORG</sub></mark>Universal  ,  reported  that  the  island  's <mark class=\"x1\" style=\"background-color: rgb(166, 226, 45);\">Justice <sub class=\"x2\">ORG</sub></mark>Department  had  issued  search  warrants  to  confiscate  the  cellphones  of  several  people  who  took  part  in  the  private  chats  .  Hundreds  of  thousands  of <mark class=\"x1\" style=\"background-color: rgb(166, 226, 45);\">Puerto <sub class=\"x2\">LOC</sub></mark><mark class=\"x1\" style=\"background-color: rgb(166, 226, 45);\">Ricans <sub class=\"x2\">MISC</sub></mark>have  been  protesting  for  12  consecutive  days  ,  demanding <mark class=\"x1\" style=\"background-color: rgb(166, 226, 45);\">Rosselló <sub class=\"x2\">PER</sub></mark>'s  ouster  .  Protests  continued  to  grow  on  the  island  after <mark class=\"x1\" style=\"background-color: rgb(166, 226, 45);\">Rosselló <sub class=\"x2\">PER</sub></mark>announced  on  Sunday  that  he  would  n't  run  for  re-election  and  that  he  would  step  down  from  the  leadership  of  the  pro-statehood  New  Progressive  Party  .  On  Monday  ,  more  than  a  half-million  people  paralyzed  metropolitan <mark class=\"x1\" style=\"background-color: rgb(166, 226, 45);\">San <sub class=\"x2\">LOC</sub></mark><mark class=\"x1\" style=\"background-color: rgb(166, 226, 45);\">Juan <sub class=\"x2\">PER</sub></mark>in  protest  ,  marching  across  one  of  the  main  highways  despite  heavy  rain  in  a  ``  March  of  the  People  ''  that  ended  late  in  the  night  as  police  fired  tear  gas  canisters  . "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0VpTSmDiGF6Z",
        "colab_type": "code",
        "outputId": "b253216c-d06d-4fcf-961f-59823467d11a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        }
      },
      "source": [
        "from render_results import render_predictions\n",
        "render_predictions(predict_examples, result, id2label)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The Puerto<B-LOC> Rican<B-MISC> Legislature gave Gov .\n",
            "Ricardo<B-PER> Rosselló<B-PER> a choice by saying that they 're ready to start impeachment proceedings unless he resigns first .\n",
            "The president of Puerto<B-LOC> Rico<B-LOC> ’ s House of Representatives , Carlos<B-PER> Méndez<B-PER> Núñez<B-PER> , has called for an extraordinary legislative session to officially begin an impeachment process against Rosselló<B-PER> Thursday afternoon .\n",
            "The news came on Wednesday after three attorneys , commissioned by Méndez<B-PER> Núñez<B-PER> , unanimously determined there were five offenses , stemming from leaked scandalous chats , that constitute grounds for impeachment .\n",
            "In the report that was leaked to the press , the attorneys found Rosselló<B-PER> committed four serious offenses and one misdemeanor , including illicitly using public resources and services for partisan purposes , as well as allowing government officials and contractors to misuse public funds and time for non-government work .\n",
            "During a press conference , Méndez<B-PER> Núñez<B-PER> said the only thing that would stop the process is if the governor resigns .\n",
            "If Rosselló<B-PER> steps down , then the report would be passed on to the relevant authorities , he added .\n",
            "On Wednesday evening , a crowd of thousands had gathered in Old San<B-LOC> Juan<B-PER> near the governor 's mansion , shouting `` Ricky<B-PER> , Renuncia<B-LOC> ! '' One of the protesters , Adrián<B-PER> , told NBC<B-ORG> News that `` tonight was supposed to be a dance party .\n",
            "'' But since Rosselló<B-PER> has consistently refused to resign , he 's afraid things will get ugly .\n",
            "“ This will be bad , ” said Adrián<B-PER> , who brought his gas mask and goggles as a precaution measure .\n",
            "On Wednesday evening , a large number of reporters were assembled for hours at the door of the governor 's mansion , waiting for what was expected to be a much earlier announcement from Rosselló<B-PER> or his aides about his future .\n",
            "Rosselló<B-PER> 's public affairs secretary , Anthony<B-PER> Maceira<B-LOC> , addressed reporters two hours later , but only said that the governor was working on a statement that he would give at an unspecified time .\n",
            "The turmoil follows the island 's largest protest in recent history calling for Rosselló<B-PER> 's ouster over scandals involving leaked private chats as well as corruption investigations and arrests .\n",
            "News of Rosselló<B-PER> 's impeachment process came a day after NBC<B-ORG> News and Telemundo<B-ORG> , both owned by NBC<B-ORG> Universal , reported that the island 's Justice<B-ORG> Department had issued search warrants to confiscate the cellphones of several people who took part in the private chats .\n",
            "Hundreds of thousands of Puerto<B-LOC> Ricans<B-MISC> have been protesting for 12 consecutive days , demanding Rosselló<B-PER> 's ouster .\n",
            "Protests continued to grow on the island after Rosselló<B-PER> announced on Sunday that he would n't run for re-election and that he would step down from the leadership of the pro-statehood New Progressive Party .\n",
            "On Monday , more than a half-million people paralyzed metropolitan San<B-LOC> Juan<B-PER> in protest , marching across one of the main highways despite heavy rain in a `` March of the People '' that ended late in the night as police fired tear gas canisters .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ki0AlU0g39GP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ec8f7eeb-0ad2-4ab2-84f9-0320fd58f67f"
      },
      "source": [
        "for (id, lbl) in id2label.items():\n",
        "  print(f'{id}={lbl}')\n",
        "\n",
        "for pe, r in zip(predict_examples, result):\n",
        "  length = len(pe.tokenized_tokens)\n",
        "  tgs = list(map(lambda x: id2label[x], r[1:length + 1]))\n",
        "  print(f'{str(pe.tokens):<18}|{str(pe.tokenized_tokens):<35}|{str(pe.tokenized_labels):<20}|{str(tgs):<20}')\n",
        "print(predict_examples[6].tokens)\n",
        "print(result[6])"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1=O\n",
            "2=B-PER\n",
            "3=I-PER\n",
            "4=B-ORG\n",
            "5=I-ORG\n",
            "6=B-LOC\n",
            "7=I-LOC\n",
            "8=B-MISC\n",
            "9=I-MISC\n",
            "10=X\n",
            "['The']           |['The']                            |['O']               |['O']               \n",
            "['Puerto']        |['Puerto']                         |['O']               |['B-LOC']           \n",
            "['Rican']         |['Rican']                          |['O']               |['B-MISC']          \n",
            "['Legislature']   |['Legislature']                    |['O']               |['O']               \n",
            "['gave']          |['gave']                           |['O']               |['O']               \n",
            "['Gov']           |['Go', '##v']                      |['O', 'X']          |['O', 'X']          \n",
            "['.']             |['.']                              |['O']               |['O']               \n",
            "['Ricardo']       |['Ricardo']                        |['O']               |['B-PER']           \n",
            "['Rosselló']      |['Ross', '##ell', '##ó']           |['O', 'X', 'X']     |['B-PER', 'X', 'X'] \n",
            "['a']             |['a']                              |['O']               |['O']               \n",
            "['choice']        |['choice']                         |['O']               |['O']               \n",
            "['by']            |['by']                             |['O']               |['O']               \n",
            "['saying']        |['saying']                         |['O']               |['O']               \n",
            "['that']          |['that']                           |['O']               |['O']               \n",
            "['they']          |['they']                           |['O']               |['O']               \n",
            "[\"'re\"]           |[\"'\", 're']                        |['O', 'X']          |['O', 'O']          \n",
            "['ready']         |['ready']                          |['O']               |['O']               \n",
            "['to']            |['to']                             |['O']               |['O']               \n",
            "['start']         |['start']                          |['O']               |['O']               \n",
            "['impeachment']   |['imp', '##each', '##ment']        |['O', 'X', 'X']     |['O', 'X', 'X']     \n",
            "['proceedings']   |['proceedings']                    |['O']               |['O']               \n",
            "['unless']        |['unless']                         |['O']               |['O']               \n",
            "['he']            |['he']                             |['O']               |['O']               \n",
            "['resigns']       |['resign', '##s']                  |['O', 'X']          |['O', 'X']          \n",
            "['first']         |['first']                          |['O']               |['O']               \n",
            "['.']             |['.']                              |['O']               |['O']               \n",
            "['The']           |['The']                            |['O']               |['O']               \n",
            "['president']     |['president']                      |['O']               |['O']               \n",
            "['of']            |['of']                             |['O']               |['O']               \n",
            "['Puerto']        |['Puerto']                         |['O']               |['B-LOC']           \n",
            "['Rico']          |['Rico']                           |['O']               |['B-LOC']           \n",
            "['’']             |['’']                              |['O']               |['O']               \n",
            "['s']             |['s']                              |['O']               |['O']               \n",
            "['House']         |['House']                          |['O']               |['O']               \n",
            "['of']            |['of']                             |['O']               |['O']               \n",
            "['Representatives']|['Representatives']                |['O']               |['O']               \n",
            "[',']             |[',']                              |['O']               |['O']               \n",
            "['Carlos']        |['Carlos']                         |['O']               |['B-PER']           \n",
            "['Méndez']        |['M', '##én', '##dez']             |['O', 'X', 'X']     |['B-PER', 'X', 'X'] \n",
            "['Núñez']         |['N', '##ú', '##ñez']              |['O', 'X', 'X']     |['B-PER', 'X', 'X'] \n",
            "[',']             |[',']                              |['O']               |['O']               \n",
            "['has']           |['has']                            |['O']               |['O']               \n",
            "['called']        |['called']                         |['O']               |['O']               \n",
            "['for']           |['for']                            |['O']               |['O']               \n",
            "['an']            |['an']                             |['O']               |['O']               \n",
            "['extraordinary'] |['extraordinary']                  |['O']               |['O']               \n",
            "['legislative']   |['legislative']                    |['O']               |['O']               \n",
            "['session']       |['session']                        |['O']               |['O']               \n",
            "['to']            |['to']                             |['O']               |['O']               \n",
            "['officially']    |['officially']                     |['O']               |['O']               \n",
            "['begin']         |['begin']                          |['O']               |['O']               \n",
            "['an']            |['an']                             |['O']               |['O']               \n",
            "['impeachment']   |['imp', '##each', '##ment']        |['O', 'X', 'X']     |['O', 'X', 'X']     \n",
            "['process']       |['process']                        |['O']               |['O']               \n",
            "['against']       |['against']                        |['O']               |['O']               \n",
            "['Rosselló']      |['Ross', '##ell', '##ó']           |['O', 'X', 'X']     |['B-PER', 'X', 'X'] \n",
            "['Thursday']      |['Thursday']                       |['O']               |['O']               \n",
            "['afternoon']     |['afternoon']                      |['O']               |['O']               \n",
            "['.']             |['.']                              |['O']               |['O']               \n",
            "['The']           |['The']                            |['O']               |['O']               \n",
            "['news']          |['news']                           |['O']               |['O']               \n",
            "['came']          |['came']                           |['O']               |['O']               \n",
            "['on']            |['on']                             |['O']               |['O']               \n",
            "['Wednesday']     |['Wednesday']                      |['O']               |['O']               \n",
            "['after']         |['after']                          |['O']               |['O']               \n",
            "['three']         |['three']                          |['O']               |['O']               \n",
            "['attorneys']     |['attorneys']                      |['O']               |['O']               \n",
            "[',']             |[',']                              |['O']               |['O']               \n",
            "['commissioned']  |['commissioned']                   |['O']               |['O']               \n",
            "['by']            |['by']                             |['O']               |['O']               \n",
            "['Méndez']        |['M', '##én', '##dez']             |['O', 'X', 'X']     |['B-PER', 'X', 'X'] \n",
            "['Núñez']         |['N', '##ú', '##ñez']              |['O', 'X', 'X']     |['B-PER', 'X', 'X'] \n",
            "[',']             |[',']                              |['O']               |['O']               \n",
            "['unanimously']   |['unanimously']                    |['O']               |['O']               \n",
            "['determined']    |['determined']                     |['O']               |['O']               \n",
            "['there']         |['there']                          |['O']               |['O']               \n",
            "['were']          |['were']                           |['O']               |['O']               \n",
            "['five']          |['five']                           |['O']               |['O']               \n",
            "['offenses']      |['offense', '##s']                 |['O', 'X']          |['O', 'X']          \n",
            "[',']             |[',']                              |['O']               |['O']               \n",
            "['stemming']      |['stem', '##ming']                 |['O', 'X']          |['O', 'X']          \n",
            "['from']          |['from']                           |['O']               |['O']               \n",
            "['leaked']        |['leaked']                         |['O']               |['O']               \n",
            "['scandalous']    |['scandal', '##ous']               |['O', 'X']          |['O', 'X']          \n",
            "['chats']         |['chat', '##s']                    |['O', 'X']          |['O', 'X']          \n",
            "[',']             |[',']                              |['O']               |['O']               \n",
            "['that']          |['that']                           |['O']               |['O']               \n",
            "['constitute']    |['constitute']                     |['O']               |['O']               \n",
            "['grounds']       |['grounds']                        |['O']               |['O']               \n",
            "['for']           |['for']                            |['O']               |['O']               \n",
            "['impeachment']   |['imp', '##each', '##ment']        |['O', 'X', 'X']     |['O', 'X', 'X']     \n",
            "['.']             |['.']                              |['O']               |['O']               \n",
            "['In']            |['In']                             |['O']               |['O']               \n",
            "['the']           |['the']                            |['O']               |['O']               \n",
            "['report']        |['report']                         |['O']               |['O']               \n",
            "['that']          |['that']                           |['O']               |['O']               \n",
            "['was']           |['was']                            |['O']               |['O']               \n",
            "['leaked']        |['leaked']                         |['O']               |['O']               \n",
            "['to']            |['to']                             |['O']               |['O']               \n",
            "['the']           |['the']                            |['O']               |['O']               \n",
            "['press']         |['press']                          |['O']               |['O']               \n",
            "[',']             |[',']                              |['O']               |['O']               \n",
            "['the']           |['the']                            |['O']               |['O']               \n",
            "['attorneys']     |['attorneys']                      |['O']               |['O']               \n",
            "['found']         |['found']                          |['O']               |['O']               \n",
            "['Rosselló']      |['Ross', '##ell', '##ó']           |['O', 'X', 'X']     |['B-PER', 'X', 'X'] \n",
            "['committed']     |['committed']                      |['O']               |['O']               \n",
            "['four']          |['four']                           |['O']               |['O']               \n",
            "['serious']       |['serious']                        |['O']               |['O']               \n",
            "['offenses']      |['offense', '##s']                 |['O', 'X']          |['O', 'X']          \n",
            "['and']           |['and']                            |['O']               |['O']               \n",
            "['one']           |['one']                            |['O']               |['O']               \n",
            "['misdemeanor']   |['mi', '##s', '##de', '##me', '##ano', '##r']|['O', 'X', 'X', 'X', 'X', 'X']|['O', 'X', 'X', 'X', 'X', 'X']\n",
            "[',']             |[',']                              |['O']               |['O']               \n",
            "['including']     |['including']                      |['O']               |['O']               \n",
            "['illicitly']     |['ill', '##icit', '##ly']          |['O', 'X', 'X']     |['O', 'X', 'X']     \n",
            "['using']         |['using']                          |['O']               |['O']               \n",
            "['public']        |['public']                         |['O']               |['O']               \n",
            "['resources']     |['resources']                      |['O']               |['O']               \n",
            "['and']           |['and']                            |['O']               |['O']               \n",
            "['services']      |['services']                       |['O']               |['O']               \n",
            "['for']           |['for']                            |['O']               |['O']               \n",
            "['partisan']      |['partisan']                       |['O']               |['O']               \n",
            "['purposes']      |['purposes']                       |['O']               |['O']               \n",
            "[',']             |[',']                              |['O']               |['O']               \n",
            "['as']            |['as']                             |['O']               |['O']               \n",
            "['well']          |['well']                           |['O']               |['O']               \n",
            "['as']            |['as']                             |['O']               |['O']               \n",
            "['allowing']      |['allowing']                       |['O']               |['O']               \n",
            "['government']    |['government']                     |['O']               |['O']               \n",
            "['officials']     |['officials']                      |['O']               |['O']               \n",
            "['and']           |['and']                            |['O']               |['O']               \n",
            "['contractors']   |['contractors']                    |['O']               |['O']               \n",
            "['to']            |['to']                             |['O']               |['O']               \n",
            "['misuse']        |['mi', '##suse']                   |['O', 'X']          |['O', 'X']          \n",
            "['public']        |['public']                         |['O']               |['O']               \n",
            "['funds']         |['funds']                          |['O']               |['O']               \n",
            "['and']           |['and']                            |['O']               |['O']               \n",
            "['time']          |['time']                           |['O']               |['O']               \n",
            "['for']           |['for']                            |['O']               |['O']               \n",
            "['non-government']|['non', '-', 'government']         |['O', 'X', 'X']     |['O', 'X', 'X']     \n",
            "['work']          |['work']                           |['O']               |['O']               \n",
            "['.']             |['.']                              |['O']               |['O']               \n",
            "['During']        |['During']                         |['O']               |['O']               \n",
            "['a']             |['a']                              |['O']               |['O']               \n",
            "['press']         |['press']                          |['O']               |['O']               \n",
            "['conference']    |['conference']                     |['O']               |['O']               \n",
            "[',']             |[',']                              |['O']               |['O']               \n",
            "['Méndez']        |['M', '##én', '##dez']             |['O', 'X', 'X']     |['B-PER', 'X', 'X'] \n",
            "['Núñez']         |['N', '##ú', '##ñez']              |['O', 'X', 'X']     |['B-PER', 'X', 'X'] \n",
            "['said']          |['said']                           |['O']               |['O']               \n",
            "['the']           |['the']                            |['O']               |['O']               \n",
            "['only']          |['only']                           |['O']               |['O']               \n",
            "['thing']         |['thing']                          |['O']               |['O']               \n",
            "['that']          |['that']                           |['O']               |['O']               \n",
            "['would']         |['would']                          |['O']               |['O']               \n",
            "['stop']          |['stop']                           |['O']               |['O']               \n",
            "['the']           |['the']                            |['O']               |['O']               \n",
            "['process']       |['process']                        |['O']               |['O']               \n",
            "['is']            |['is']                             |['O']               |['O']               \n",
            "['if']            |['if']                             |['O']               |['O']               \n",
            "['the']           |['the']                            |['O']               |['O']               \n",
            "['governor']      |['governor']                       |['O']               |['O']               \n",
            "['resigns']       |['resign', '##s']                  |['O', 'X']          |['O', 'X']          \n",
            "['.']             |['.']                              |['O']               |['O']               \n",
            "['If']            |['If']                             |['O']               |['O']               \n",
            "['Rosselló']      |['Ross', '##ell', '##ó']           |['O', 'X', 'X']     |['B-PER', 'X', 'X'] \n",
            "['steps']         |['steps']                          |['O']               |['O']               \n",
            "['down']          |['down']                           |['O']               |['O']               \n",
            "[',']             |[',']                              |['O']               |['O']               \n",
            "['then']          |['then']                           |['O']               |['O']               \n",
            "['the']           |['the']                            |['O']               |['O']               \n",
            "['report']        |['report']                         |['O']               |['O']               \n",
            "['would']         |['would']                          |['O']               |['O']               \n",
            "['be']            |['be']                             |['O']               |['O']               \n",
            "['passed']        |['passed']                         |['O']               |['O']               \n",
            "['on']            |['on']                             |['O']               |['O']               \n",
            "['to']            |['to']                             |['O']               |['O']               \n",
            "['the']           |['the']                            |['O']               |['O']               \n",
            "['relevant']      |['relevant']                       |['O']               |['O']               \n",
            "['authorities']   |['authorities']                    |['O']               |['O']               \n",
            "[',']             |[',']                              |['O']               |['O']               \n",
            "['he']            |['he']                             |['O']               |['O']               \n",
            "['added']         |['added']                          |['O']               |['O']               \n",
            "['.']             |['.']                              |['O']               |['O']               \n",
            "['On']            |['On']                             |['O']               |['O']               \n",
            "['Wednesday']     |['Wednesday']                      |['O']               |['O']               \n",
            "['evening']       |['evening']                        |['O']               |['O']               \n",
            "[',']             |[',']                              |['O']               |['O']               \n",
            "['a']             |['a']                              |['O']               |['O']               \n",
            "['crowd']         |['crowd']                          |['O']               |['O']               \n",
            "['of']            |['of']                             |['O']               |['O']               \n",
            "['thousands']     |['thousands']                      |['O']               |['O']               \n",
            "['had']           |['had']                            |['O']               |['O']               \n",
            "['gathered']      |['gathered']                       |['O']               |['O']               \n",
            "['in']            |['in']                             |['O']               |['O']               \n",
            "['Old']           |['Old']                            |['O']               |['O']               \n",
            "['San']           |['San']                            |['O']               |['B-LOC']           \n",
            "['Juan']          |['Juan']                           |['O']               |['B-PER']           \n",
            "['near']          |['near']                           |['O']               |['O']               \n",
            "['the']           |['the']                            |['O']               |['O']               \n",
            "['governor']      |['governor']                       |['O']               |['O']               \n",
            "[\"'s\"]            |[\"'\", 's']                         |['O', 'X']          |['O', 'X']          \n",
            "['mansion']       |['mansion']                        |['O']               |['O']               \n",
            "[',']             |[',']                              |['O']               |['O']               \n",
            "['shouting']      |['shouting']                       |['O']               |['O']               \n",
            "['``']            |['`', '`']                         |['O', 'X']          |['O', 'X']          \n",
            "['Ricky']         |['Ricky']                          |['O']               |['B-PER']           \n",
            "[',']             |[',']                              |['O']               |['O']               \n",
            "['Renuncia']      |['Ren', '##un', '##cia']           |['O', 'X', 'X']     |['B-LOC', 'X', 'X'] \n",
            "['!']             |['!']                              |['O']               |['O']               \n",
            "[\"''\"]            |[\"'\", \"'\"]                         |['O', 'X']          |['O', 'O']          \n",
            "['One']           |['One']                            |['O']               |['O']               \n",
            "['of']            |['of']                             |['O']               |['O']               \n",
            "['the']           |['the']                            |['O']               |['O']               \n",
            "['protesters']    |['protesters']                     |['O']               |['O']               \n",
            "[',']             |[',']                              |['O']               |['O']               \n",
            "['Adrián']        |['Ad', '##ri', '##án']             |['O', 'X', 'X']     |['B-PER', 'X', 'X'] \n",
            "[',']             |[',']                              |['O']               |['O']               \n",
            "['told']          |['told']                           |['O']               |['O']               \n",
            "['NBC']           |['NBC']                            |['O']               |['B-ORG']           \n",
            "['News']          |['News']                           |['O']               |['O']               \n",
            "['that']          |['that']                           |['O']               |['O']               \n",
            "['``']            |['`', '`']                         |['O', 'X']          |['O', 'X']          \n",
            "['tonight']       |['tonight']                        |['O']               |['O']               \n",
            "['was']           |['was']                            |['O']               |['O']               \n",
            "['supposed']      |['supposed']                       |['O']               |['O']               \n",
            "['to']            |['to']                             |['O']               |['O']               \n",
            "['be']            |['be']                             |['O']               |['O']               \n",
            "['a']             |['a']                              |['O']               |['O']               \n",
            "['dance']         |['dance']                          |['O']               |['O']               \n",
            "['party']         |['party']                          |['O']               |['O']               \n",
            "['.']             |['.']                              |['O']               |['O']               \n",
            "[\"''\"]            |[\"'\", \"'\"]                         |['O', 'X']          |['O', 'O']          \n",
            "['But']           |['But']                            |['O']               |['O']               \n",
            "['since']         |['since']                          |['O']               |['O']               \n",
            "['Rosselló']      |['Ross', '##ell', '##ó']           |['O', 'X', 'X']     |['B-PER', 'X', 'X'] \n",
            "['has']           |['has']                            |['O']               |['O']               \n",
            "['consistently']  |['consistently']                   |['O']               |['O']               \n",
            "['refused']       |['refused']                        |['O']               |['O']               \n",
            "['to']            |['to']                             |['O']               |['O']               \n",
            "['resign']        |['resign']                         |['O']               |['O']               \n",
            "[',']             |[',']                              |['O']               |['O']               \n",
            "['he']            |['he']                             |['O']               |['O']               \n",
            "[\"'s\"]            |[\"'\", 's']                         |['O', 'X']          |['O', 'X']          \n",
            "['afraid']        |['afraid']                         |['O']               |['O']               \n",
            "['things']        |['things']                         |['O']               |['O']               \n",
            "['will']          |['will']                           |['O']               |['O']               \n",
            "['get']           |['get']                            |['O']               |['O']               \n",
            "['ugly']          |['ugly']                           |['O']               |['O']               \n",
            "['.']             |['.']                              |['O']               |['O']               \n",
            "['“']             |['“']                              |['O']               |['O']               \n",
            "['This']          |['This']                           |['O']               |['O']               \n",
            "['will']          |['will']                           |['O']               |['O']               \n",
            "['be']            |['be']                             |['O']               |['O']               \n",
            "['bad']           |['bad']                            |['O']               |['O']               \n",
            "[',']             |[',']                              |['O']               |['O']               \n",
            "['”']             |['”']                              |['O']               |['O']               \n",
            "['said']          |['said']                           |['O']               |['O']               \n",
            "['Adrián']        |['Ad', '##ri', '##án']             |['O', 'X', 'X']     |['B-PER', 'X', 'X'] \n",
            "[',']             |[',']                              |['O']               |['O']               \n",
            "['who']           |['who']                            |['O']               |['O']               \n",
            "['brought']       |['brought']                        |['O']               |['O']               \n",
            "['his']           |['his']                            |['O']               |['O']               \n",
            "['gas']           |['gas']                            |['O']               |['O']               \n",
            "['mask']          |['mask']                           |['O']               |['O']               \n",
            "['and']           |['and']                            |['O']               |['O']               \n",
            "['goggles']       |['go', '##ggles']                  |['O', 'X']          |['O', 'X']          \n",
            "['as']            |['as']                             |['O']               |['O']               \n",
            "['a']             |['a']                              |['O']               |['O']               \n",
            "['precaution']    |['pre', '##ca', '##ution']         |['O', 'X', 'X']     |['O', 'X', 'X']     \n",
            "['measure']       |['measure']                        |['O']               |['O']               \n",
            "['.']             |['.']                              |['O']               |['O']               \n",
            "['On']            |['On']                             |['O']               |['O']               \n",
            "['Wednesday']     |['Wednesday']                      |['O']               |['O']               \n",
            "['evening']       |['evening']                        |['O']               |['O']               \n",
            "[',']             |[',']                              |['O']               |['O']               \n",
            "['a']             |['a']                              |['O']               |['O']               \n",
            "['large']         |['large']                          |['O']               |['O']               \n",
            "['number']        |['number']                         |['O']               |['O']               \n",
            "['of']            |['of']                             |['O']               |['O']               \n",
            "['reporters']     |['reporters']                      |['O']               |['O']               \n",
            "['were']          |['were']                           |['O']               |['O']               \n",
            "['assembled']     |['assembled']                      |['O']               |['O']               \n",
            "['for']           |['for']                            |['O']               |['O']               \n",
            "['hours']         |['hours']                          |['O']               |['O']               \n",
            "['at']            |['at']                             |['O']               |['O']               \n",
            "['the']           |['the']                            |['O']               |['O']               \n",
            "['door']          |['door']                           |['O']               |['O']               \n",
            "['of']            |['of']                             |['O']               |['O']               \n",
            "['the']           |['the']                            |['O']               |['O']               \n",
            "['governor']      |['governor']                       |['O']               |['O']               \n",
            "[\"'s\"]            |[\"'\", 's']                         |['O', 'X']          |['O', 'X']          \n",
            "['mansion']       |['mansion']                        |['O']               |['O']               \n",
            "[',']             |[',']                              |['O']               |['O']               \n",
            "['waiting']       |['waiting']                        |['O']               |['O']               \n",
            "['for']           |['for']                            |['O']               |['O']               \n",
            "['what']          |['what']                           |['O']               |['O']               \n",
            "['was']           |['was']                            |['O']               |['O']               \n",
            "['expected']      |['expected']                       |['O']               |['O']               \n",
            "['to']            |['to']                             |['O']               |['O']               \n",
            "['be']            |['be']                             |['O']               |['O']               \n",
            "['a']             |['a']                              |['O']               |['O']               \n",
            "['much']          |['much']                           |['O']               |['O']               \n",
            "['earlier']       |['earlier']                        |['O']               |['O']               \n",
            "['announcement']  |['announcement']                   |['O']               |['O']               \n",
            "['from']          |['from']                           |['O']               |['O']               \n",
            "['Rosselló']      |['Ross', '##ell', '##ó']           |['O', 'X', 'X']     |['B-PER', 'X', 'X'] \n",
            "['or']            |['or']                             |['O']               |['O']               \n",
            "['his']           |['his']                            |['O']               |['O']               \n",
            "['aides']         |['aide', '##s']                    |['O', 'X']          |['O', 'X']          \n",
            "['about']         |['about']                          |['O']               |['O']               \n",
            "['his']           |['his']                            |['O']               |['O']               \n",
            "['future']        |['future']                         |['O']               |['O']               \n",
            "['.']             |['.']                              |['O']               |['O']               \n",
            "['Rosselló']      |['Ross', '##ell', '##ó']           |['O', 'X', 'X']     |['B-PER', 'X', 'X'] \n",
            "[\"'s\"]            |[\"'\", 's']                         |['O', 'X']          |['O', 'X']          \n",
            "['public']        |['public']                         |['O']               |['O']               \n",
            "['affairs']       |['affairs']                        |['O']               |['O']               \n",
            "['secretary']     |['secretary']                      |['O']               |['O']               \n",
            "[',']             |[',']                              |['O']               |['O']               \n",
            "['Anthony']       |['Anthony']                        |['O']               |['B-PER']           \n",
            "['Maceira']       |['Mace', '##ira']                  |['O', 'X']          |['B-LOC', 'X']      \n",
            "[',']             |[',']                              |['O']               |['O']               \n",
            "['addressed']     |['addressed']                      |['O']               |['O']               \n",
            "['reporters']     |['reporters']                      |['O']               |['O']               \n",
            "['two']           |['two']                            |['O']               |['O']               \n",
            "['hours']         |['hours']                          |['O']               |['O']               \n",
            "['later']         |['later']                          |['O']               |['O']               \n",
            "[',']             |[',']                              |['O']               |['O']               \n",
            "['but']           |['but']                            |['O']               |['O']               \n",
            "['only']          |['only']                           |['O']               |['O']               \n",
            "['said']          |['said']                           |['O']               |['O']               \n",
            "['that']          |['that']                           |['O']               |['O']               \n",
            "['the']           |['the']                            |['O']               |['O']               \n",
            "['governor']      |['governor']                       |['O']               |['O']               \n",
            "['was']           |['was']                            |['O']               |['O']               \n",
            "['working']       |['working']                        |['O']               |['O']               \n",
            "['on']            |['on']                             |['O']               |['O']               \n",
            "['a']             |['a']                              |['O']               |['O']               \n",
            "['statement']     |['statement']                      |['O']               |['O']               \n",
            "['that']          |['that']                           |['O']               |['O']               \n",
            "['he']            |['he']                             |['O']               |['O']               \n",
            "['would']         |['would']                          |['O']               |['O']               \n",
            "['give']          |['give']                           |['O']               |['O']               \n",
            "['at']            |['at']                             |['O']               |['O']               \n",
            "['an']            |['an']                             |['O']               |['O']               \n",
            "['unspecified']   |['un', '##sp', '##ec', '##ified']  |['O', 'X', 'X', 'X']|['O', 'X', 'X', 'X']\n",
            "['time']          |['time']                           |['O']               |['O']               \n",
            "['.']             |['.']                              |['O']               |['O']               \n",
            "['The']           |['The']                            |['O']               |['O']               \n",
            "['turmoil']       |['turmoil']                        |['O']               |['O']               \n",
            "['follows']       |['follows']                        |['O']               |['O']               \n",
            "['the']           |['the']                            |['O']               |['O']               \n",
            "['island']        |['island']                         |['O']               |['O']               \n",
            "[\"'s\"]            |[\"'\", 's']                         |['O', 'X']          |['O', 'X']          \n",
            "['largest']       |['largest']                        |['O']               |['O']               \n",
            "['protest']       |['protest']                        |['O']               |['O']               \n",
            "['in']            |['in']                             |['O']               |['O']               \n",
            "['recent']        |['recent']                         |['O']               |['O']               \n",
            "['history']       |['history']                        |['O']               |['O']               \n",
            "['calling']       |['calling']                        |['O']               |['O']               \n",
            "['for']           |['for']                            |['O']               |['O']               \n",
            "['Rosselló']      |['Ross', '##ell', '##ó']           |['O', 'X', 'X']     |['B-PER', 'X', 'X'] \n",
            "[\"'s\"]            |[\"'\", 's']                         |['O', 'X']          |['O', 'X']          \n",
            "['ouster']        |['ou', '##ster']                   |['O', 'X']          |['O', 'X']          \n",
            "['over']          |['over']                           |['O']               |['O']               \n",
            "['scandals']      |['scandal', '##s']                 |['O', 'X']          |['O', 'X']          \n",
            "['involving']     |['involving']                      |['O']               |['O']               \n",
            "['leaked']        |['leaked']                         |['O']               |['O']               \n",
            "['private']       |['private']                        |['O']               |['O']               \n",
            "['chats']         |['chat', '##s']                    |['O', 'X']          |['O', 'X']          \n",
            "['as']            |['as']                             |['O']               |['O']               \n",
            "['well']          |['well']                           |['O']               |['O']               \n",
            "['as']            |['as']                             |['O']               |['O']               \n",
            "['corruption']    |['corruption']                     |['O']               |['O']               \n",
            "['investigations']|['investigations']                 |['O']               |['O']               \n",
            "['and']           |['and']                            |['O']               |['O']               \n",
            "['arrests']       |['arrests']                        |['O']               |['O']               \n",
            "['.']             |['.']                              |['O']               |['O']               \n",
            "['News']          |['News']                           |['O']               |['O']               \n",
            "['of']            |['of']                             |['O']               |['O']               \n",
            "['Rosselló']      |['Ross', '##ell', '##ó']           |['O', 'X', 'X']     |['B-PER', 'X', 'X'] \n",
            "[\"'s\"]            |[\"'\", 's']                         |['O', 'X']          |['O', 'X']          \n",
            "['impeachment']   |['imp', '##each', '##ment']        |['O', 'X', 'X']     |['O', 'X', 'X']     \n",
            "['process']       |['process']                        |['O']               |['O']               \n",
            "['came']          |['came']                           |['O']               |['O']               \n",
            "['a']             |['a']                              |['O']               |['O']               \n",
            "['day']           |['day']                            |['O']               |['O']               \n",
            "['after']         |['after']                          |['O']               |['O']               \n",
            "['NBC']           |['NBC']                            |['O']               |['B-ORG']           \n",
            "['News']          |['News']                           |['O']               |['O']               \n",
            "['and']           |['and']                            |['O']               |['O']               \n",
            "['Telemundo']     |['Tel', '##em', '##und', '##o']    |['O', 'X', 'X', 'X']|['B-ORG', 'X', 'X', 'X']\n",
            "[',']             |[',']                              |['O']               |['O']               \n",
            "['both']          |['both']                           |['O']               |['O']               \n",
            "['owned']         |['owned']                          |['O']               |['O']               \n",
            "['by']            |['by']                             |['O']               |['O']               \n",
            "['NBC']           |['NBC']                            |['O']               |['B-ORG']           \n",
            "['Universal']     |['Universal']                      |['O']               |['O']               \n",
            "[',']             |[',']                              |['O']               |['O']               \n",
            "['reported']      |['reported']                       |['O']               |['O']               \n",
            "['that']          |['that']                           |['O']               |['O']               \n",
            "['the']           |['the']                            |['O']               |['O']               \n",
            "['island']        |['island']                         |['O']               |['O']               \n",
            "[\"'s\"]            |[\"'\", 's']                         |['O', 'X']          |['O', 'X']          \n",
            "['Justice']       |['Justice']                        |['O']               |['B-ORG']           \n",
            "['Department']    |['Department']                     |['O']               |['O']               \n",
            "['had']           |['had']                            |['O']               |['O']               \n",
            "['issued']        |['issued']                         |['O']               |['O']               \n",
            "['search']        |['search']                         |['O']               |['O']               \n",
            "['warrants']      |['warrant', '##s']                 |['O', 'X']          |['O', 'X']          \n",
            "['to']            |['to']                             |['O']               |['O']               \n",
            "['confiscate']    |['con', '##fi', '##sca', '##te']   |['O', 'X', 'X', 'X']|['O', 'X', 'X', 'X']\n",
            "['the']           |['the']                            |['O']               |['O']               \n",
            "['cellphones']    |['cell', '##phones']               |['O', 'X']          |['O', 'X']          \n",
            "['of']            |['of']                             |['O']               |['O']               \n",
            "['several']       |['several']                        |['O']               |['O']               \n",
            "['people']        |['people']                         |['O']               |['O']               \n",
            "['who']           |['who']                            |['O']               |['O']               \n",
            "['took']          |['took']                           |['O']               |['O']               \n",
            "['part']          |['part']                           |['O']               |['O']               \n",
            "['in']            |['in']                             |['O']               |['O']               \n",
            "['the']           |['the']                            |['O']               |['O']               \n",
            "['private']       |['private']                        |['O']               |['O']               \n",
            "['chats']         |['chat', '##s']                    |['O', 'X']          |['O', 'X']          \n",
            "['.']             |['.']                              |['O']               |['O']               \n",
            "['Hundreds']      |['Hundreds']                       |['O']               |['O']               \n",
            "['of']            |['of']                             |['O']               |['O']               \n",
            "['thousands']     |['thousands']                      |['O']               |['O']               \n",
            "['of']            |['of']                             |['O']               |['O']               \n",
            "['Puerto']        |['Puerto']                         |['O']               |['B-LOC']           \n",
            "['Ricans']        |['Rican', '##s']                   |['O', 'X']          |['B-MISC', 'X']     \n",
            "['have']          |['have']                           |['O']               |['O']               \n",
            "['been']          |['been']                           |['O']               |['O']               \n",
            "['protesting']    |['protesting']                     |['O']               |['O']               \n",
            "['for']           |['for']                            |['O']               |['O']               \n",
            "['12']            |['12']                             |['O']               |['O']               \n",
            "['consecutive']   |['consecutive']                    |['O']               |['O']               \n",
            "['days']          |['days']                           |['O']               |['O']               \n",
            "[',']             |[',']                              |['O']               |['O']               \n",
            "['demanding']     |['demanding']                      |['O']               |['O']               \n",
            "['Rosselló']      |['Ross', '##ell', '##ó']           |['O', 'X', 'X']     |['B-PER', 'X', 'X'] \n",
            "[\"'s\"]            |[\"'\", 's']                         |['O', 'X']          |['O', 'X']          \n",
            "['ouster']        |['ou', '##ster']                   |['O', 'X']          |['O', 'X']          \n",
            "['.']             |['.']                              |['O']               |['O']               \n",
            "['Protests']      |['Pro', '##test', '##s']           |['O', 'X', 'X']     |['O', 'X', 'X']     \n",
            "['continued']     |['continued']                      |['O']               |['O']               \n",
            "['to']            |['to']                             |['O']               |['O']               \n",
            "['grow']          |['grow']                           |['O']               |['O']               \n",
            "['on']            |['on']                             |['O']               |['O']               \n",
            "['the']           |['the']                            |['O']               |['O']               \n",
            "['island']        |['island']                         |['O']               |['O']               \n",
            "['after']         |['after']                          |['O']               |['O']               \n",
            "['Rosselló']      |['Ross', '##ell', '##ó']           |['O', 'X', 'X']     |['B-PER', 'X', 'X'] \n",
            "['announced']     |['announced']                      |['O']               |['O']               \n",
            "['on']            |['on']                             |['O']               |['O']               \n",
            "['Sunday']        |['Sunday']                         |['O']               |['O']               \n",
            "['that']          |['that']                           |['O']               |['O']               \n",
            "['he']            |['he']                             |['O']               |['O']               \n",
            "['would']         |['would']                          |['O']               |['O']               \n",
            "[\"n't\"]           |['n', \"'\", 't']                    |['O', 'X', 'X']     |['O', 'X', 'X']     \n",
            "['run']           |['run']                            |['O']               |['O']               \n",
            "['for']           |['for']                            |['O']               |['O']               \n",
            "['re-election']   |['re', '-', 'election']            |['O', 'X', 'X']     |['O', 'X', 'X']     \n",
            "['and']           |['and']                            |['O']               |['O']               \n",
            "['that']          |['that']                           |['O']               |['O']               \n",
            "['he']            |['he']                             |['O']               |['O']               \n",
            "['would']         |['would']                          |['O']               |['O']               \n",
            "['step']          |['step']                           |['O']               |['O']               \n",
            "['down']          |['down']                           |['O']               |['O']               \n",
            "['from']          |['from']                           |['O']               |['O']               \n",
            "['the']           |['the']                            |['O']               |['O']               \n",
            "['leadership']    |['leadership']                     |['O']               |['O']               \n",
            "['of']            |['of']                             |['O']               |['O']               \n",
            "['the']           |['the']                            |['O']               |['O']               \n",
            "['pro-statehood'] |['pro', '-', 'state', '##hood']    |['O', 'X', 'X', 'X']|['O', 'X', 'X', 'X']\n",
            "['New']           |['New']                            |['O']               |['O']               \n",
            "['Progressive']   |['Progressive']                    |['O']               |['O']               \n",
            "['Party']         |['Party']                          |['O']               |['O']               \n",
            "['.']             |['.']                              |['O']               |['O']               \n",
            "['On']            |['On']                             |['O']               |['O']               \n",
            "['Monday']        |['Monday']                         |['O']               |['O']               \n",
            "[',']             |[',']                              |['O']               |['O']               \n",
            "['more']          |['more']                           |['O']               |['O']               \n",
            "['than']          |['than']                           |['O']               |['O']               \n",
            "['a']             |['a']                              |['O']               |['O']               \n",
            "['half-million']  |['half', '-', 'million']           |['O', 'X', 'X']     |['O', 'X', 'X']     \n",
            "['people']        |['people']                         |['O']               |['O']               \n",
            "['paralyzed']     |['paralyzed']                      |['O']               |['O']               \n",
            "['metropolitan']  |['metropolitan']                   |['O']               |['O']               \n",
            "['San']           |['San']                            |['O']               |['B-LOC']           \n",
            "['Juan']          |['Juan']                           |['O']               |['B-PER']           \n",
            "['in']            |['in']                             |['O']               |['O']               \n",
            "['protest']       |['protest']                        |['O']               |['O']               \n",
            "[',']             |[',']                              |['O']               |['O']               \n",
            "['marching']      |['marching']                       |['O']               |['O']               \n",
            "['across']        |['across']                         |['O']               |['O']               \n",
            "['one']           |['one']                            |['O']               |['O']               \n",
            "['of']            |['of']                             |['O']               |['O']               \n",
            "['the']           |['the']                            |['O']               |['O']               \n",
            "['main']          |['main']                           |['O']               |['O']               \n",
            "['highways']      |['highways']                       |['O']               |['O']               \n",
            "['despite']       |['despite']                        |['O']               |['O']               \n",
            "['heavy']         |['heavy']                          |['O']               |['O']               \n",
            "['rain']          |['rain']                           |['O']               |['O']               \n",
            "['in']            |['in']                             |['O']               |['O']               \n",
            "['a']             |['a']                              |['O']               |['O']               \n",
            "['``']            |['`', '`']                         |['O', 'X']          |['O', 'X']          \n",
            "['March']         |['March']                          |['O']               |['O']               \n",
            "['of']            |['of']                             |['O']               |['O']               \n",
            "['the']           |['the']                            |['O']               |['O']               \n",
            "['People']        |['People']                         |['O']               |['O']               \n",
            "[\"''\"]            |[\"'\", \"'\"]                         |['O', 'X']          |['O', 'O']          \n",
            "['that']          |['that']                           |['O']               |['O']               \n",
            "['ended']         |['ended']                          |['O']               |['O']               \n",
            "['late']          |['late']                           |['O']               |['O']               \n",
            "['in']            |['in']                             |['O']               |['O']               \n",
            "['the']           |['the']                            |['O']               |['O']               \n",
            "['night']         |['night']                          |['O']               |['O']               \n",
            "['as']            |['as']                             |['O']               |['O']               \n",
            "['police']        |['police']                         |['O']               |['O']               \n",
            "['fired']         |['fired']                          |['O']               |['O']               \n",
            "['tear']          |['tear']                           |['O']               |['O']               \n",
            "['gas']           |['gas']                            |['O']               |['O']               \n",
            "['canisters']     |['can', '##ister', '##s']          |['O', 'X', 'X']     |['O', 'X', 'X']     \n",
            "['.']             |['.']                              |['O']               |['O']               \n",
            "['.']\n",
            "[0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YbkP17IfKDJn",
        "colab_type": "code",
        "outputId": "640b12b9-9f12-4958-f6a1-97963546eb5d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "#print(FLAGS.__dict__['__wrapped'])\n",
        "flags.DEFINE_string(\"f\", 'f', \"f\")\n",
        "FLAGS.data_dir"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/bert-ner/CoNLL/data'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sx6ZrFKxnc9K",
        "colab_type": "code",
        "outputId": "76f3fdcd-a2fe-4115-9cbd-7280469441dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!ps ax | grep python\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "#my_sent = \"Hi man, how have you been?\"\n",
        "#print(word_tokenize(my_sent))\n",
        "\n",
        "ls = sent_tokenize(text)\n",
        "for (i,l) in enumerate(ls): \n",
        "  print(f'====== line: {i} ============')\n",
        "  print(l)\n",
        "\n",
        "\n",
        "\n",
        "#!zip model.ckpt-2632.zip \"/content/drive/My Drive/ner_checkpoints/model.ckpt-2000.data-00000-of-00001\" \"/content/drive/My Drive/ner_checkpoints/model.ckpt-2632.index\" \"/content/drive/My Drive/ner_checkpoints/model.ckpt-2632.meta\" \"/content/drive/My Drive/ner_checkpoints/label2id.pkl\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "     28 ?        Sl     0:03 /usr/bin/python2 /usr/local/bin/jupyter-notebook --ip=\"172.28.0.2\" --port=9000 --FileContentsManager.root_dir=\"/\" --MappingKernelManager.root_dir=\"/content\"\n",
            "    133 ?        Ssl    0:07 /usr/bin/python3 -m ipykernel_launcher -f /root/.local/share/jupyter/runtime/kernel-317cbf9d-4c91-465b-bf56-800c930e6f59.json\n",
            "    422 ?        S      0:00 /usr/bin/python3 -Wignore:::pip._internal.cli.base_command -c from multiprocessing.semaphore_tracker import main;main(58)\n",
            "    546 ?        S      0:00 /bin/bash -c ps ax | grep python\n",
            "    548 ?        S      0:00 grep python\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "====== line: 0 ============\n",
            "\n",
            "INTERLOCAL AGREEMENT FOR CONSULTING SERVICES\n",
            "\n",
            "This is an Interlocal Agreement entered into pursuant to the authority of ch.\n",
            "====== line: 1 ============\n",
            "39.34 RCW, between the Cities of Lacey, Olympia, Tumwater, and Thurston County, hereinafter referred to as the JURISDICTIONS, for the purpose of sharing the collective cost of negotiating with Comcast Cable for the renewal of each entity’s cable franchise.\n",
            "====== line: 2 ============\n",
            "All parties agree to the terms outlined herein:\n",
            "\n",
            "\n",
            "    1.\n",
            "====== line: 3 ============\n",
            "The JURISDICTIONS agree that it would be mutually beneficial to negotiate with Comcast Cable jointly rather than individually for the purposes of renewing each entity’s cable franchise.\n",
            "====== line: 4 ============\n",
            "By doing so, the cost of negotiating with Comcast Cable will be incrementally reduced for each party.\n",
            "====== line: 5 ============\n",
            "2.\n",
            "====== line: 6 ============\n",
            "The total cost, including but not limited to consultant fees and legal review necessary to finalize cable franchise negotiations with Comcast Cable for the parties involved, will be paid by the JURISDICTIONS on a per subscriber basis, according the provisions of Paragraph 3, herein.\n",
            "====== line: 7 ============\n",
            "3.\n",
            "====== line: 8 ============\n",
            "In order to allocate the initial distribution of the costs of negotiations on a per subscriber basis, the JURISDICTIONS agree with the subscriber data outlined in Attachment “A.”   For example, bBased on the current subscriber data in Attachment “A”, Thurston County will pay 51%, Lacey  16%, Olympia  25%, and Tumwater  8% of the total costs for the consultant hired pursuant to Paragraph 4, herein, subject to the termination provisions of Paragraph 11, herein.\n",
            "====== line: 9 ============\n",
            "Attachment “A” will be updated at the beginning of each year with the most current cable subscribers.\n",
            "====== line: 10 ============\n",
            "This update will represent an amendment to this Agreement, pursuant to Paragraph 10, herein.\n",
            "====== line: 11 ============\n",
            "Each such update (amendment) will act to supersede the prior Attachment A, in order to continue the distribution of the cost of franchise renewal on a per subscriber basis.\n",
            "====== line: 12 ============\n",
            "4.\n",
            "====== line: 13 ============\n",
            "The JURISDICTIONS will jointly produce a Request for Proposals (RFP), to include a Scope of Work, for purposes of hiring a consultant (or a consulting firm) to negotiate with Comcast Cable on behalf of the JURISDICTIONS.\n",
            "====== line: 14 ============\n",
            "A draft Scope of Work is attached to this Agreement, as Attachment B.\n",
            "====== line: 15 ============\n",
            "The Scope of Work will be divided into two phases: (1) Review and Recommendation; and (2) Negotiation and Final Franchise Agreement.\n",
            "====== line: 16 ============\n",
            "The draft Scope of Work is subject to change, depending upon the negotiation process with the consultant (or firm) that is hired.\n",
            "====== line: 17 ============\n",
            "The final Scope of Work will be incorporated into the professional services agreement with the consultant that is hired through the RFP process.\n",
            "====== line: 18 ============\n",
            "All jurisdictions JURISDICTIONS must agree to the final Scope of Work, by letter indicating such agreement, addressed to John Tennis, Public Information Program Manager, Thurston County, prior to award of the consultant contract by Thurston County.\n",
            "====== line: 19 ============\n",
            "5.\n",
            "====== line: 20 ============\n",
            "The term of this Agreement shall be effective upon the approval of each party’s governing body, and recording of the Agreement with the Thurston County Auditor.\n",
            "====== line: 21 ============\n",
            "This Agreement shall terminate on June 30, 2007 unless extended by agreement of the parties.\n",
            "====== line: 22 ============\n",
            "6.\n",
            "====== line: 23 ============\n",
            "Thurston County will act as the Lead Agency on behalf of the JURISDICTIONS.\n",
            "====== line: 24 ============\n",
            "The Lead Agency, for the purpose of this Agreement, will award the contract for cable consultant as well as collect payments under this Agreement and administer payments to satisfy the consultant contract.\n",
            "====== line: 25 ============\n",
            "Thurston County rules and procedures will be followed for the purpose of awarding the contract.\n",
            "====== line: 26 ============\n",
            "[The last sentence in this paragraph doesn’t make sense.\n",
            "====== line: 27 ============\n",
            "If Thurston County is going to be the lead agency and award the contract, then each jurisdiction’s governing body doesn’t need to execute the contract with the consultant, as this Agreement provides the basis for one jurisdiction to award the contract.]\n",
            "====== line: 28 ============\n",
            "Thurston County Comment:  In order for the County to agree to execute the consultant contract on behalf of the Jurisdictions, the following language will need to replace the currently proposed section 6 language:\n",
            "\n",
            "Thurston County will act as the Lead Agency on behalf of the JURISDICTIONS.\n",
            "====== line: 29 ============\n",
            "The Lead Agency, for the purpose of this Agreement, will award and execute the contract for cable consultant, collect payments under this Agreement, and administer the consultant contract.\n",
            "====== line: 30 ============\n",
            "Thurston County procedures will be followed for the purpose of awarding and executing the consultant contract.\n",
            "====== line: 31 ============\n",
            "It is understood by the parties to this Agreement that Thurston County is executing and administering the cable consultant contract on behalf of the JURISDICTIONS and each JURISDICTION shall be responsible under the cable consultant contract as if each JURISDICTION individually executed said contract.\n",
            "====== line: 32 ============\n",
            "The responsibility of the JURISDICTIONS under this section shall survive the termination of this Agreement whether termination is by all parties, or by one or more parties.\n",
            "====== line: 33 ============\n",
            "7.\n",
            "====== line: 34 ============\n",
            "The parties agree that each of the parties shall, at all times, be solely responsible for the acts or the failure to act of its personnel that occur or arise in any way out of the performance of this Agreement by its personnel only, and to save and hold the other parties and their personnel and officials harmless from all costs, expenses, losses and damages, including cost of defense, incurred as a result of any acts or omissions of the party’s personnel relating to the performance of this Agreement.\n",
            "====== line: 35 ============\n",
            "The provisions of this section shall survive termination of this Agreement, whether termination is by all parties, or by one or more parties.\n",
            "====== line: 36 ============\n",
            "8.\n",
            "====== line: 37 ============\n",
            "The parties shall not jointly acquire property and therefore there is no need to set forth a means of disposition of such property.\n",
            "====== line: 38 ============\n",
            "9.\n",
            "====== line: 39 ============\n",
            "No separate legal entity is created by this Agreement.\n",
            "====== line: 40 ============\n",
            "No administrator or joint board is created by this Agreement; however, Thurston County will be the lead agency for the purposes set forth in Paragraph 6, herein.\n",
            "====== line: 41 ============\n",
            "10.\n",
            "====== line: 42 ============\n",
            "This Agreement may only be modified by mutual agreement of all parties hereto, executed in writing, in the same manner as this Agreement.\n",
            "====== line: 43 ============\n",
            "11.\n",
            "====== line: 44 ============\n",
            "This Agreement may be terminated as to any single party, when the party provides notice to all other parties in writing at least 60 days prior to its intended withdrawal from this Agreement.\n",
            "====== line: 45 ============\n",
            "Upon the effective date of termination by a party, the distribution of costs shall be reallocated between the non-terminating parties as set forth in Paragraphs 2 and 3, herein, and will constitute an amendment to this Agreement pursuant to the terms of Paragraph 10, herein.\n",
            "====== line: 46 ============\n",
            "Any and all costs incurred or encumbered by the parties prior to the effective date of termination of a party, will remain the responsibility of the withdrawing party.\n",
            "====== line: 47 ============\n",
            "Thurston County Comment:  By adding the highlighted language to section 11, this would mean that all jurisdictions would need to mutually agree to the termination.\n",
            "====== line: 48 ============\n",
            "Is this what is contemplated by the jurisdictions?\n",
            "====== line: 49 ============\n",
            "If not, then this language should not be included.\n",
            "====== line: 50 ============\n",
            "12.\n",
            "====== line: 51 ============\n",
            "Notice provided for in this Agreement shall be sent by certified mail to the addresses as set forth below.\n",
            "====== line: 52 ============\n",
            "Notice will be deemed received three business days following posting by the U.S. Postmaster.\n",
            "====== line: 53 ============\n",
            "City of Tumwater, c/o City Administrator, 555 Israel Road SW, Tumwater, WA  98501.\n",
            "====== line: 54 ============\n",
            "City of Olympia [Add name and address here.]\n",
            "====== line: 55 ============\n",
            "City of Lacey [Add name and address here.]\n",
            "====== line: 56 ============\n",
            "Thurston County [Add name and address here.]\n",
            "====== line: 57 ============\n",
            "13.\n",
            "====== line: 58 ============\n",
            "This Agreement has been and shall be construed as having been made and delivered within the State of Washington, and it is agreed by each party hereto that this Agreement shall be governed by laws of the State of Washington, both as to interpretation and performance.\n",
            "====== line: 59 ============\n",
            "Any action of law, suit in equity, or judicial proceeding for the enforcement of this Agreement or any provisions thereof, shall be instituted and maintained only in Superior Court in Thurston County, Washington.\n",
            "====== line: 60 ============\n",
            "14.\n",
            "====== line: 61 ============\n",
            "If, for any reason, any part, term or provision of this Agreement is held by a court of competent jurisdiction to be illegal, void or unenforceable, the validity of the remaining provisions shall not be affected, and the rights and obligations of the parties shall be construed and enforced as if the Agreement did not contain the particular provision held to be invalid.\n",
            "====== line: 62 ============\n",
            "15.\n",
            "====== line: 63 ============\n",
            "The parties agree that this Agreement is the complete expression of the terms hereto and any oral representations or understandings not incorporated herein are excluded.\n",
            "====== line: 64 ============\n",
            "It is also agreed by the parties that the forgiveness of the nonperformance of any provision of this Agreement does not constitute a waiver of the provisions of this Agreement.\n",
            "====== line: 65 ============\n",
            "IN WITNESS WHEREOF the parties hereto have caused this Agreement to be executed according to the terms written above.\n",
            "====== line: 66 ============\n",
            "City of Lacey:\t\t\t\t\t\tCity of Olympia:\n",
            "\n",
            "By:\t\t\t\t\t\t\tBy:\t\t\t\t\t\n",
            "\n",
            "Its:\t\t\t\t\t\t\tIts:\t\t\t\t\t\n",
            "\n",
            "Date:\t\t\t\t\t\t\tDate:\t\t\t\t\t\n",
            "\n",
            "Attest:\t\t\t\t\t\t\tAttest:\n",
            "\n",
            "By:\t\t\t\t\t\t\tBy:\t\t\t\t\t\n",
            "\n",
            "Approved as to form:\t\t\t\t\tApproved as to form:\n",
            "\n",
            "By:\t\t\t\t\t\t\tBy:\t\t\t\t\t\n",
            "\n",
            "City of Tumwater:\t\t\t\t\tThurston County:\n",
            "\n",
            "\t\t\t\t\t\t\t:\t\t\t\t\t\n",
            "Ralph C. Osgood, Mayor\t\t\t\tChairman\n",
            "\t\t\t\t\t\t\t\t\t\t\t\t\n",
            "\t\t\t\t\t\t\tVice-Chairman\n",
            "\t\t\t\t\t\t\t______________________________\n",
            "\t\t\t\t\t\t\tCommissioner\n",
            "Date:\t\t\t\t\t\t\tDate:\t\t\t\t\t\n",
            "\n",
            "Attest:\t\t\t\t\t\t\tAttest:\n",
            "\n",
            "By:\t\t\t\t\t\t\tBy:\t\t\t\t\t\n",
            "\n",
            "Approved as to form:\t\t\t\t\tApproved as to form:\n",
            "\t\t\t\t\t\t\tBy:\t\t\t\t\t\n",
            "Christy A. Todd, City Attorney\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m0kDpZntrJBX",
        "colab_type": "code",
        "outputId": "1c04f796-7ab5-4981-a036-74729023eedc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        }
      },
      "source": [
        "from IPython.display import HTML, display\n",
        "xxx = \"\"\"\n",
        "<style> \n",
        ".x1 {\n",
        "\t  padding: .2em .3em;\n",
        "    padding-top: 0.2em;\n",
        "    padding-right: 0.3em;\n",
        "    padding-bottom: 0.2em;\n",
        "    padding-left: 0.3em;\n",
        "    margin: 0 .25em;\n",
        "    margin-top: 0px;\n",
        "    margin-right: 0.25em;\n",
        "    margin-bottom: 0px;\n",
        "    margin-left: 0.25em;\n",
        "    line-height: 1;\n",
        "    display: inline-block;\n",
        "    border-radius: .25em;\n",
        "    border-top-left-radius: 0.25em;\n",
        "    border-top-right-radius: 0.25em;\n",
        "    border-bottom-right-radius: 0.25em;\n",
        "    border-bottom-left-radius: 0.25em;\n",
        "}\n",
        "\n",
        ".x2 {\n",
        "    box-sizing: border-box;\n",
        "    content: attr(data-entity);\n",
        "    font-size: .55em;\n",
        "    line-height: 1;\n",
        "    padding: .35em .35em;\n",
        "    padding-top: 0.35em;\n",
        "    padding-right: 0.35em;\n",
        "    padding-bottom: 0.35em;\n",
        "    padding-left: 0.35em;\n",
        "    border-radius: .35em;\n",
        "    text-transform: uppercase;\n",
        "    display: inline-block;\n",
        "    vertical-align: middle;\n",
        "    margin: 0 0 .15rem .5rem;\n",
        "    margin-top: 0px;\n",
        "    margin-right: 0px;\n",
        "    margin-bottom: 0.15rem;\n",
        "    margin-left: 0.5rem;\n",
        "    background: #fff;\n",
        "    background-image: initial;\n",
        "    background-position-x: initial;\n",
        "    background-position-y: initial;\n",
        "    background-size: initial;\n",
        "    background-repeat-x: initial;\n",
        "    background-repeat-y: initial;\n",
        "    background-attachment: initial;\n",
        "    background-origin: initial;\n",
        "    background-clip: initial;\n",
        "    background-color: rgb(255, 255, 255);\n",
        "    font-weight: 700;\n",
        "}\n",
        "\n",
        "\n",
        "</style>\n",
        "\n",
        "<p>Rounded corners for an element with a specified background color:</p>kljhkhlkj\n",
        "<mark class=\"x1\" style='background-color: red;'>Rounded corners!<sub class=\"x2\">PERS</sub></mark>\n",
        "<p>Rounded corners for an element with a border:</p>\n",
        "<p id=\"rcorners2\">Rounded corners!</p>\n",
        "<p>Rounded corners for an element with a background image:</p>\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "display(HTML(xxx), metadata=dict(isolated=True))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "<style> \n",
              ".x1 {\n",
              "\t  padding: .2em .3em;\n",
              "    padding-top: 0.2em;\n",
              "    padding-right: 0.3em;\n",
              "    padding-bottom: 0.2em;\n",
              "    padding-left: 0.3em;\n",
              "    margin: 0 .25em;\n",
              "    margin-top: 0px;\n",
              "    margin-right: 0.25em;\n",
              "    margin-bottom: 0px;\n",
              "    margin-left: 0.25em;\n",
              "    line-height: 1;\n",
              "    display: inline-block;\n",
              "    border-radius: .25em;\n",
              "    border-top-left-radius: 0.25em;\n",
              "    border-top-right-radius: 0.25em;\n",
              "    border-bottom-right-radius: 0.25em;\n",
              "    border-bottom-left-radius: 0.25em;\n",
              "}\n",
              "\n",
              ".x2 {\n",
              "    box-sizing: border-box;\n",
              "    content: attr(data-entity);\n",
              "    font-size: .55em;\n",
              "    line-height: 1;\n",
              "    padding: .35em .35em;\n",
              "    padding-top: 0.35em;\n",
              "    padding-right: 0.35em;\n",
              "    padding-bottom: 0.35em;\n",
              "    padding-left: 0.35em;\n",
              "    border-radius: .35em;\n",
              "    text-transform: uppercase;\n",
              "    display: inline-block;\n",
              "    vertical-align: middle;\n",
              "    margin: 0 0 .15rem .5rem;\n",
              "    margin-top: 0px;\n",
              "    margin-right: 0px;\n",
              "    margin-bottom: 0.15rem;\n",
              "    margin-left: 0.5rem;\n",
              "    background: #fff;\n",
              "    background-image: initial;\n",
              "    background-position-x: initial;\n",
              "    background-position-y: initial;\n",
              "    background-size: initial;\n",
              "    background-repeat-x: initial;\n",
              "    background-repeat-y: initial;\n",
              "    background-attachment: initial;\n",
              "    background-origin: initial;\n",
              "    background-clip: initial;\n",
              "    background-color: rgb(255, 255, 255);\n",
              "    font-weight: 700;\n",
              "}\n",
              "\n",
              "\n",
              "</style>\n",
              "\n",
              "<p>Rounded corners for an element with a specified background color:</p>kljhkhlkj\n",
              "<mark class=\"x1\" style='background-color: red;'>Rounded corners!<sub class=\"x2\">PERS</sub></mark>\n",
              "<p>Rounded corners for an element with a border:</p>\n",
              "<p id=\"rcorners2\">Rounded corners!</p>\n",
              "<p>Rounded corners for an element with a background image:</p>\n",
              "\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": [],
            "isolated": true
          }
        }
      ]
    }
  ]
}